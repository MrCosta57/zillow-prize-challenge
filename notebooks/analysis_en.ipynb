{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DWM PROJECT 2021\n",
        "### John Coast - 880892"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Index:\n",
        "- [Analisi del dataset \"Train\"](#analisi_train)\n",
        "- [Analisi del dataset \"Properties\"](#analisi_prop)\n",
        "- [Features Engineering](#features_engineer)\n",
        "- [Gestione dei missing values e rimozione delle colonne non necessarie o che presentano multicollinearità](#missing_val)\n",
        "- [Recupero di missing values per features connesse a posizione geografica e tassazione](#recover_missing_pos_tax)\n",
        "- [Aggiunta di features custom potenzialmente utili](#custom_features)\n",
        "- [Features importance, features selection e preparazione del dataset finale](#features_selection)\n",
        "- [Corstruzione del modello sfruttante Random Forest e tuning dei suoi parametri](#mod_1)\n",
        "- [Corstruzione del modello sfruttante Gradient Boosting e tuning dei suoi parametri](#mod_2)\n",
        "- [Comparazione e analisi dei modelli](#comparison_and_analisys)\n",
        "- [Considerazioni finali](#final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Library imports\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from utils.general_utils import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings, random, joblib, os\n",
        "import xgboost as xgb\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Set up the working directory\n",
        "INPUT_DATA_DIR = \"../data/input\"\n",
        "OUTPUT_DATA_DIR = \"../data/output\"\n",
        "MODEL_DIR = \"models\"\n",
        "PROPS_FILENAME = \"properties_2016.parquet\"\n",
        "TRAIN_DATA_FILENAME = \"train_2016_v2.csv\"\n",
        "PROPS_PATH = os.path.join(INPUT_DATA_DIR, PROPS_FILENAME)\n",
        "TRAIN_DATA_PATH = os.path.join(INPUT_DATA_DIR, TRAIN_DATA_FILENAME)\n",
        "SEED = 42\n",
        "TOP_K_DISPLAY = 20\n",
        "\n",
        "# Set up the random seed\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "# Display options\n",
        "pd.options.display.float_format = '{:.4f}'.format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='analisi_train'></a>\n",
        "### Analysis of the \"Train\" dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\n",
        "    TRAIN_DATA_PATH, parse_dates=[\"transactiondate\"], date_format=\"%Y-%m-%d\"\n",
        ")\n",
        "df_train[df_train.select_dtypes(np.float64).columns] = df_train.select_dtypes(\n",
        "    np.float64\n",
        ").astype(np.float32)\n",
        "df_train.info(max_cols=TOP_K_DISPLAY)\n",
        "print(\"Shape: \", df_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing value ratio\n",
        "df_train.isna().sum() / df_train.shape[0] * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We note that the dataset contains 3 columns:\n",
        "- Parcelid: unique ID that identifies each building instance\n",
        "- Logerror: index that we should use to verify the goodness of our model<br>\n",
        "From the competition website: *\"Zillow is asking you to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. The log error is defined as logerror=log(Zestimate)−log( SalePrice)\"*\n",
        "- Transactiondate: Actual or estimated sale date for that building instance\n",
        "\n",
        "Also there are no null values ​​in the dataset but there are duplicate parcelids, even though the parcelid and transactiondate pair is unique for each row. This implies that there are actual (or forecasted) sales data for the same building on different days"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='analysis_prop'></a>\n",
        "### Analysis of the \"Properties\" dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop = pd.DataFrame([])\n",
        "if PROPS_PATH.endswith(\".csv\"):\n",
        "    df_prop = pd.read_csv(PROPS_PATH)\n",
        "    to_float64_float32(df_prop)\n",
        "    df_prop.to_parquet(\n",
        "        os.path.join(INPUT_DATA_DIR, PROPS_FILENAME.split(\".\")[0] + \".parquet\")\n",
        "    )\n",
        "else:\n",
        "    df_prop = pd.read_parquet(PROPS_PATH)\n",
        "\n",
        "# df_prop=pd.read_csv(input_folder+data_file_name, header=0) #parameter nrows=x to limit the number of rows\n",
        "\n",
        "df_prop.info(max_cols=TOP_K_DISPLAY)\n",
        "print(\"Shape: \", df_prop.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing value ratio\n",
        "df_prop.isna().sum().sort_values(ascending=False)[:TOP_K_DISPLAY] / df_prop.shape[\n",
        "    0\n",
        "] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values ​​plot\n",
        "df_missing = df_prop.isnull().sum(axis=0).reset_index()\n",
        "df_missing.columns = [\"column_name\", \"missing_count\"]\n",
        "df_missing = df_missing.loc[df_missing[\"missing_count\"] > 0]\n",
        "df_missing = df_missing.sort_values(by=\"missing_count\")\n",
        "\n",
        "ind = np.arange(df_missing.shape[0])\n",
        "fig, ax = plt.subplots(figsize=(12, 18))\n",
        "rects = ax.barh(ind, df_missing[\"missing_count\"])\n",
        "ax.set_yticks(ind)\n",
        "ax.set_yticklabels(df_missing[\"column_name\"], rotation=\"horizontal\")\n",
        "ax.set_xlabel(\"Count of missing values\")\n",
        "ax.set_title(\"Number of missing values in each column\")\n",
        "plt.show()\n",
        "del df_missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Columns containing categorical variables based on the documentation provided on kaggle.com (used later)\n",
        "other_cols = [\n",
        "    \"parcelid\",\n",
        "    \"airconditioningtypeid\",\n",
        "    \"architecturalstyletypeid\",\n",
        "    \"buildingqualitytypeid\",\n",
        "    \"buildingclasstypeid\",\n",
        "    \"decktypeid\",\n",
        "    \"fips\",\n",
        "    \"fireplaceflag\",\n",
        "    \"hashottuborspa\",\n",
        "    \"heatingorsystemtypeid\",\n",
        "    \"pooltypeid10\",\n",
        "    \"pooltypeid2\",\n",
        "    \"pooltypeid7\",\n",
        "    \"propertycountylandusecode\",\n",
        "    \"propertylandusetypeid\",\n",
        "    \"propertyzoningdesc\",\n",
        "    \"rawcensustractandblock\",\n",
        "    \"censustractandblock\",\n",
        "    \"regionidcounty\",\n",
        "    \"regionidcity\",\n",
        "    \"regionidzip\",\n",
        "    \"regionidneighborhood\",\n",
        "    \"regionidzip\",\n",
        "    \"storytypeid\",\n",
        "    \"typeconstructiontypeid\",\n",
        "    \"assessmentyear\",\n",
        "    \"taxdelinquencyflag\",\n",
        "    \"taxdelinquencyyear\",\n",
        "    \"yearbuilt\",\n",
        "]\n",
        "numerical_cols = [x for x in df_prop.columns if x not in other_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Displaying column values ​​of non-float, int, or datetime type\n",
        "not_float_col = df_prop.select_dtypes(exclude=[np.float32, np.int64]).columns\n",
        "for c in not_float_col:\n",
        "    print(\"Column: \" + c)\n",
        "    print(\"values: \", df_prop[c].unique()[:TOP_K_DISPLAY], \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset has many features and instances:\n",
        "- some of these having type float32 are actually numeric identifiers of integer type associated with categories described in more detail in the Kaggle.com documentation\n",
        "- those of type object instead are strings representing boolean values ​​or codes relating to categories\n",
        "\n",
        "The various columns will then be managed subsequently to represent the correct semantics of the data.\n",
        "\n",
        "The dataset also does not present duplicates either in parcelid or globally, but it contains a very high number of missing values: in fact, several columns present a missing ratio higher than 97%. In the following chapters, it will be illustrated how these missing values ​​are managed for each of the features present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this particular supervised learning task, only the houses inside the \"train_2016_v2\" file are sufficient and not all those present in \"properties_2016\", so we proceed to merge the two datasets previously analyzed in order to have a single one containing the set of all the columns and only the rows that will actually be used for the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop.info(max_cols=TOP_K_DISPLAY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset after merge:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop = df_train.merge(df_prop, on=\"parcelid\")\n",
        "df_prop.info(max_cols=TOP_K_DISPLAY)\n",
        "print(\"Shape: \", df_prop.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='features_engineer'></a>\n",
        "## Features Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='missing_val'></a>\n",
        "### Handling missing values ​​and removing unnecessary or multicollinear columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap to visualize correlations between variables\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(data=df_prop[numerical_cols].corr(), cmap=\"Reds\")\n",
        "plt.show()\n",
        "plt.gcf().clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the analysis of the heatmap on the correlation between the numerical variables, it is noted that the features 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15' and 'finishedsquarefeet6' are highly correlated being dark red in color.\n",
        "The same consideration also applies to 'finishedfloor1squarefeet' and 'finishedsquarefeet50' which additionally have the same description in the documentation provided.\n",
        "Similarly 'bathroomcnt', 'calculatedbathnbr' and 'fullbathcnt' are also related."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Only 'calculatedfinishedsquarefeet' is kept among the four because it has fewer missing values, while between 'finishedfloor1squarefeet' and 'finishedsquarefeet50' 'finishedsquarefeet50' is arbitrarily removed because the missing ratio is equivalent.\n",
        "\n",
        "'bathroomcnt' and 'calculatedbathnbr' are also removed, leaving 'bathroomcnt', as a similar intuition to the previous one is applied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop[\n",
        "    [\n",
        "        \"calculatedfinishedsquarefeet\",\n",
        "        \"finishedsquarefeet12\",\n",
        "        \"finishedsquarefeet13\",\n",
        "        \"finishedsquarefeet15\",\n",
        "        \"finishedsquarefeet6\",\n",
        "    ]\n",
        "].isna().sum() / df_prop.shape[0] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop.drop(\n",
        "    columns=[\n",
        "        \"finishedsquarefeet12\",\n",
        "        \"finishedsquarefeet13\",\n",
        "        \"finishedsquarefeet15\",\n",
        "        \"finishedsquarefeet6\",\n",
        "    ],\n",
        "    inplace=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop[\n",
        "    [\"finishedfloor1squarefeet\", \"finishedsquarefeet50\"]\n",
        "].isna().sum() / df_prop.shape[0] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop.drop(columns=\"finishedsquarefeet50\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop[\n",
        "    [\"calculatedbathnbr\", \"bathroomcnt\", \"fullbathcnt\"]\n",
        "].isna().sum() / df_prop.shape[0] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop.drop(columns=[\"calculatedbathnbr\", \"fullbathcnt\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'hashottuborspa' and 'pooltypeid10' have semantically the same description. It is decided to remove 'pooltypeid10' which has fewer missing values.\n",
        "\n",
        "It is also assumed that if the pool/hot tub value (features 'pooltypeid2', 'pooltypeid7', 'poolcnt') is not present this indicates 0 elements present in the building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_prop[\"hashottuborspa\"].value_counts())\n",
        "print(df_prop[\"pooltypeid10\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop[[\"hashottuborspa\", \"pooltypeid10\"]].isnull().sum() / df_prop.shape[0] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop.drop(columns=\"pooltypeid10\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop[[\"pooltypeid2\", \"pooltypeid7\", \"poolcnt\"]] = df_prop[\n",
        "    [\"pooltypeid2\", \"pooltypeid7\", \"poolcnt\"]\n",
        "].fillna(0)\n",
        "df_prop[\"hashottuborspa\"] = df_prop[\"hashottuborspa\"].fillna(0)  # >90% na\n",
        "df_prop[\"hashottuborspa\"] = df_prop[\"hashottuborspa\"].astype(bool);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the 'poolsizesum' feature, the median of the values ​​present in the rows where the 'poolcnt' column is greater than 0 is used, otherwise this value is set to 0\n",
        "(we chose to use the median as a filler because it is less influenced by outliers and because we assume that the dimensions of swimming pools in the United States are more or less standard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "median_poolsize = df_prop[df_prop[\"poolcnt\"] > 0][\"poolsizesum\"].median()\n",
        "df_prop.loc[\n",
        "    (df_prop[\"poolcnt\"] > 0) & (df_prop[\"poolsizesum\"].isna()), \"poolsizesum\"\n",
        "] = median_poolsize\n",
        "\n",
        "# If you don't have a pool, the pool size is 0\n",
        "df_prop.loc[(df_prop[\"poolcnt\"] == 0), \"poolsizesum\"] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'fireplaceflag' and 'fireplacecnt' have inconsistencies:\n",
        "as you can see from the analysis below there are lines where 'fireplacecnt' is present, while 'fireplaceflag' is missing or incorrect.\n",
        "\n",
        "We then proceed by setting the values ​​of 'fireplacecnt' to 0 when NaN and setting 'fireplaceflag' to True when 'fireplacecnt' is present, False otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop[\"fireplaceflag\"] = df_prop[\"fireplaceflag\"].fillna(0)  # >90% na\n",
        "df_prop[\"fireplaceflag\"] = df_prop[\"fireplaceflag\"].astype(bool)\n",
        "df_prop.loc[(~df_prop[\"fireplacecnt\"].isna()), \"fireplaceflag\"] = True\n",
        "df_prop[\"fireplacecnt\"] = df_prop[\"fireplacecnt\"].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The values ​​of 'taxdelinquencyflag', 'garagecarcnt', 'garagetotalsqft' are filled assuming 0 if the value is NaN, as done previously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop[[\"taxdelinquencyflag\", \"garagecarcnt\", \"garagetotalsqft\"]] = df_prop[\n",
        "    [\"taxdelinquencyflag\", \"garagecarcnt\", \"garagetotalsqft\"]\n",
        "].fillna(0)\n",
        "df_prop[\"taxdelinquencyflag\"] = df_prop[\"taxdelinquencyflag\"].astype(bool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Features like 'airconditioningtypeid', 'heatingorsystemtypeid', 'threequarterbathnbr' instead are considered unimportant and variable so it is decided to replace the missing values ​​of these categorical features with their respective mode.<br>\n",
        "As can be seen from the graphs below, the frequency of these categorical variables is intuitively correct, it is reasonably assumed that AC and Heating System are more commonly 'Central' and that most homes only have a three-quarter bathroom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "df_prop[\"airconditioningtypeid\"].astype(int, errors=\"ignore\").hist(grid=False)\n",
        "plt.show()\n",
        "\n",
        "mode = float(df_prop[\"airconditioningtypeid\"].mode()[0])\n",
        "print(\"Moda: \", mode)\n",
        "df_prop[\"airconditioningtypeid\"] = df_prop[\"airconditioningtypeid\"].fillna(mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "df_prop[\"heatingorsystemtypeid\"].astype(int, errors=\"ignore\").hist(grid=False)\n",
        "plt.show()\n",
        "\n",
        "mode = float(df_prop[\"heatingorsystemtypeid\"].mode()[0])\n",
        "print(\"Moda: \", mode)\n",
        "df_prop[\"heatingorsystemtypeid\"] = df_prop[\"heatingorsystemtypeid\"].fillna(mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "df_prop[\"threequarterbathnbr\"].astype(int, errors=\"ignore\").hist(grid=False)\n",
        "plt.show()\n",
        "\n",
        "mode = float(df_prop[\"threequarterbathnbr\"].mode()[0])\n",
        "print(\"Moda: \", mode)\n",
        "df_prop[\"threequarterbathnbr\"] = df_prop[\"threequarterbathnbr\"].fillna(mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, it was decided to remove the features with a missing ratio greater than 97% because they were considered to have too little information to be useful for the regression task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmp_col = df_prop.columns\n",
        "for c in tmp_col:\n",
        "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.97:\n",
        "        df_prop.drop(columns=c, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmp_col = df_prop.columns\n",
        "tmp_list = []\n",
        "\n",
        "for c in tmp_col:\n",
        "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.0:\n",
        "        tmp_list.append(c)\n",
        "\n",
        "print(\"Number of features with missing values: \", len(tmp_list))\n",
        "print(tmp_list)\n",
        "print(\"Number of total features after droping: \", len(df_prop.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The remaining features are then analyzed, trying to restore the last missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='recover_missing_pos_tax'></a>\n",
        "### Recovering missing values ​​for features related to geographic location and taxation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geo_col_names = [\n",
        "    \"latitude\",\n",
        "    \"longitude\",\n",
        "    \"buildingqualitytypeid\",\n",
        "    \"propertycountylandusecode\",\n",
        "    \"propertyzoningdesc\",\n",
        "    \"regionidcity\",\n",
        "    \"regionidneighborhood\",\n",
        "    \"regionidzip\",\n",
        "    \"unitcnt\",\n",
        "    \"yearbuilt\",\n",
        "]\n",
        "df_geo=df_prop[geo_col_names]\n",
        "df_geo.isna().sum() / df_geo.shape[0] * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intuitively, given the absence of missing values, the 'latitude' and 'longitude' features relating to the position of the building could be used to recover other attributes that are not present: geographically close houses are in fact thought to have certain similar characteristics<br><br>\n",
        "The original values ​​of these two features are then restored because, as described in the documentation provided, those within the dataset are multiplied by 10^6.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop[\"latitude\"] = df_prop[\"latitude\"] / (10**6)\n",
        "df_prop[\"longitude\"] = df_prop[\"longitude\"] / (10**6)\n",
        "# df_prop.dropna( axis = 0, subset = ['latitude', 'longitude'], inplace = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It was decided to use the K-nearest neighbors (KNN) algorithm to perform the recovery task because it was considered the most suitable given that it is based on learning through analogies between neighboring instances and given its computational efficiency. The code cell below also explains the motivations and hypotheses used for the recovery of the categorical and numerical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop[\"buildingqualitytypeid\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "parameters = {\"n_neighbors\": [1, 2, 3, 4, 5, 8, 10]}\n",
        "\n",
        "\n",
        "# It is assumed that blocks of houses close by were all built more or less at the same time and therefore have similar quality.\n",
        "fillna_knn(\n",
        "    df=df_prop,\n",
        "    base=[\"latitude\", \"longitude\"],\n",
        "    target=\"buildingqualitytypeid\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "\n",
        "# Neighboring homes have the same countrylandusecode\n",
        "tmp_label_enc = zoningcode2int(df=df_prop, target=\"propertycountylandusecode\")\n",
        "fillna_knn(\n",
        "    df=df_prop,\n",
        "    base=[\"latitude\", \"longitude\"],\n",
        "    target=\"propertycountylandusecode\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "df_prop[\"propertycountylandusecode\"] = tmp_label_enc.inverse_transform(\n",
        "    df_prop[\"propertycountylandusecode\"].astype(int)\n",
        ")\n",
        "\n",
        "\n",
        "# Neighboring homes have the same zoning description\n",
        "tmp_label_enc = zoningcode2int(df=df_prop, target=\"propertyzoningdesc\")\n",
        "fillna_knn(\n",
        "    df=df_prop,\n",
        "    base=[\"latitude\", \"longitude\"],\n",
        "    target=\"propertyzoningdesc\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "df_prop[\"propertyzoningdesc\"] = tmp_label_enc.inverse_transform(\n",
        "    df_prop[\"propertyzoningdesc\"].astype(int)\n",
        ")\n",
        "\n",
        "\n",
        "# Here too, neighboring properties are assumed to have the same regionedcity, regionedneighborhood and regionedzip\n",
        "fillna_knn(\n",
        "    df=df_prop,\n",
        "    base=[\"latitude\", \"longitude\"],\n",
        "    target=\"regionidcity\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "\n",
        "fillna_knn(\n",
        "    df=df_prop,\n",
        "    base=[\"latitude\", \"longitude\"],\n",
        "    target=\"regionidneighborhood\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "\n",
        "fillna_knn(\n",
        "    df=df_prop,\n",
        "    base=[\"latitude\", \"longitude\"],\n",
        "    target=\"regionidzip\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "\n",
        "\n",
        "# Same intuition for the fields 'unitcnt' (Number of units the structure is built into), 'yearbuilt' and 'lotsizesquarefeet'\n",
        "fillna_knn(\n",
        "    df=df_prop,\n",
        "    base=[\"latitude\", \"longitude\"],\n",
        "    target=\"unitcnt\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "\n",
        "fillna_knn(\n",
        "    df=df_prop,\n",
        "    base=[\"latitude\", \"longitude\"],\n",
        "    target=\"yearbuilt\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "\n",
        "fillna_knn_reg(\n",
        "    df=df_prop,\n",
        "    base=[\"latitude\", \"longitude\"],\n",
        "    target=\"lotsizesquarefeet\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "\n",
        "warnings.simplefilter(action='default', category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As for the feature 'finishedfloor1squarefeet', as you can also see from the heatmap at the beginning of the current section, this is correlated with 'calculatedfinishedsquarefeet'. We therefore try to use the latter to fill its values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(13, 13))\n",
        "sns.jointplot(\n",
        "    x=df_prop[\"finishedfloor1squarefeet\"].values,\n",
        "    y=df_prop[\"calculatedfinishedsquarefeet\"].values,\n",
        ")\n",
        "plt.ylabel(\"calculatedfinishedsquarefeet\", fontsize=10)\n",
        "plt.xlabel(\"finishedfloor1squarefeet\", fontsize=10)\n",
        "plt.title(\"finishedfloor1squarefeet Vs calculatedfinishedsquarefeet\", fontsize=13)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the graph, we can see that in some houses the values ​​of the features are exactly the same: probably some houses have their total area occupied by only one room (as could be a sort of study). This information is therefore assumed to be true and the filling is carried out.<br>\n",
        "Furthermore, some rows of the dataset contain values ​​of 'finishedfloor1squarefeet' greater than the total size of the house, probably due to an incorrect input data entry. It is therefore decided to remove these rows from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop.loc[\n",
        "    (df_prop[\"finishedfloor1squarefeet\"].isna()) & (df_prop[\"numberofstories\"] == 1),\n",
        "    \"finishedfloor1squarefeet\",\n",
        "] = df_prop.loc[\n",
        "    (df_prop[\"finishedfloor1squarefeet\"].isna()) & (df_prop[\"numberofstories\"] == 1),\n",
        "    \"calculatedfinishedsquarefeet\",\n",
        "]\n",
        "\n",
        "droprows = df_prop.loc[\n",
        "    df_prop[\"calculatedfinishedsquarefeet\"] < df_prop[\"finishedfloor1squarefeet\"]\n",
        "].index\n",
        "df_prop = df_prop.drop(droprows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmp_col = df_prop.columns\n",
        "tmp_list = []\n",
        "\n",
        "for c in tmp_col:\n",
        "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.0:\n",
        "        tmp_list.append(c)\n",
        "\n",
        "print(\"Number of features with missing values: \", len(tmp_list))\n",
        "print(tmp_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to handle the variables related to building taxes: in particular we are trying to retrieve the values ​​of 'structuretaxvaluedollarcnt', 'taxamount' and 'landtaxvaluedollarcnt'.\n",
        "The variable 'taxvaluedollarcnt' is assumed to be the most significant to use as a support since it also contains the least number of missing values.<br>\n",
        "\n",
        "The NaN values ​​of 'taxvaluedollarcnt' are then filled using its median, in order to have a result less sensitive to outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tax_col_names = [\n",
        "    \"taxvaluedollarcnt\",\n",
        "    \"landtaxvaluedollarcnt\",\n",
        "    \"structuretaxvaluedollarcnt\",\n",
        "    \"taxamount\",\n",
        "]\n",
        "df_tax = df_prop[tax_col_names]\n",
        "df_tax.isna().sum() / df_tax.shape[0] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "median = df_prop[\"taxvaluedollarcnt\"].median()\n",
        "df_prop[\"taxvaluedollarcnt\"] = df_prop[\"taxvaluedollarcnt\"].fillna(median)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From correlation analysis and distribution graphs for the three target variables, it is also noted that 'taxvaluedollarcnt' is the most correlated variable for all of them: we therefore try to perform a prediction of the missing values ​​using the KNN algorithm for the features 'structuretaxvaluedollarcnt', 'taxamount' and 'landtaxvaluedollarcnt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = df_tax.corr()\n",
        "\n",
        "print(\"Target: structuretaxvaluedollarcnt\")\n",
        "print(x[\"structuretaxvaluedollarcnt\"].sort_values(ascending=False))\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.jointplot(\n",
        "    x=df_tax[\"structuretaxvaluedollarcnt\"].values, y=df_tax[\"taxvaluedollarcnt\"].values\n",
        ")\n",
        "plt.ylabel(\"taxvaluedollarcnt\", fontsize=12)\n",
        "plt.xlabel(\"structuretaxvaluedollarcnt\", fontsize=12)\n",
        "plt.title(\"structuretaxvaluedollarcnt Vs taxvaluedollarcnt\", fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "print(\"Target: taxamount\")\n",
        "print(x[\"taxamount\"].sort_values(ascending=False))\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.jointplot(x=df_tax[\"taxamount\"].values, y=df_tax[\"taxvaluedollarcnt\"].values)\n",
        "plt.ylabel(\"taxvaluedollarcnt\", fontsize=12)\n",
        "plt.xlabel(\"taxamount\", fontsize=12)\n",
        "plt.title(\"taxamount Vs taxvaluedollarcnt\", fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "print(\"Target: landtaxvaluedollarcnt\")\n",
        "print(x[\"landtaxvaluedollarcnt\"].sort_values(ascending=False))\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.jointplot(\n",
        "    x=df_tax[\"landtaxvaluedollarcnt\"].values, y=df_tax[\"taxvaluedollarcnt\"].values\n",
        ")\n",
        "plt.ylabel(\"taxvaluedollarcnt\", fontsize=12)\n",
        "plt.xlabel(\"landtaxvaluedollarcnt\", fontsize=12)\n",
        "plt.title(\"landtaxvaluedollarcnt Vs taxvaluedollarcnt\", fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = {\"n_neighbors\": [10, 20, 30, 40, 50, 100]}\n",
        "fillna_knn_reg(\n",
        "    df=df_prop,\n",
        "    base=[\"taxvaluedollarcnt\"],\n",
        "    target=\"structuretaxvaluedollarcnt\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "fillna_knn_reg(\n",
        "    df=df_prop, base=[\"taxvaluedollarcnt\"], target=\"taxamount\", tuning_params=parameters\n",
        ")\n",
        "fillna_knn_reg(\n",
        "    df=df_prop,\n",
        "    base=[\"taxvaluedollarcnt\"],\n",
        "    target=\"landtaxvaluedollarcnt\",\n",
        "    tuning_params=parameters,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmp_col = df_prop.columns\n",
        "tmp_list = []\n",
        "\n",
        "for c in tmp_col:\n",
        "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.0:\n",
        "        tmp_list.append(c)\n",
        "\n",
        "print(\"Number of features with missing values: \", len(tmp_list))\n",
        "print(tmp_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, there are only a few features left with missing values, so we proceed:\n",
        "- filling 'numberofstories' with her fashion\n",
        "- removing the 'censustractandblock' column because it is assumed that the 'rawcensustractandblock' attribute contains the same information albeit in a less elaborate way.\n",
        "- building a predictor for 'calculatedfinishedsquarefeet' based on 'bathroomcnt', 'bedroomcnt', 'structuretaxvaluedollarcnt'. The total size of the living area is assumed to depend on how many bathrooms and bedrooms there are and how much structure tax is attributed to the dwelling.\n",
        "- building a predictor for 'finishedfloor1squarefeet' based on the same features as above and the number of stories in the building (numberofstories). It is assumed that the number of stories present can be useful to calculate the feature 'finishedfloor1squarefeet' (*Dimensions of the finished living space on the first floor (entrance) of the house*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "x_min = df_prop[\"numberofstories\"].min()\n",
        "x_max = df_prop[\"numberofstories\"].max()\n",
        "plt.xlim([x_min, x_max])\n",
        "plt.xticks(np.arange(x_min, x_max + 1, 1))\n",
        "df_prop[\"numberofstories\"].hist()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "mode = float(df_prop[\"numberofstories\"].mode()[0])\n",
        "print(\"Moda: \", mode)\n",
        "df_prop[\"numberofstories\"] = df_prop[\"numberofstories\"].fillna(mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop.drop(columns=\"censustractandblock\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fillna_knn_reg(\n",
        "    df=df_prop,\n",
        "    base=[\"bathroomcnt\", \"bedroomcnt\", \"structuretaxvaluedollarcnt\"],\n",
        "    target=\"calculatedfinishedsquarefeet\",\n",
        "    tuning_params=parameters,\n",
        ")\n",
        "fillna_knn_reg(\n",
        "    df=df_prop,\n",
        "    base=[\"bathroomcnt\", \"bedroomcnt\", \"structuretaxvaluedollarcnt\", \"numberofstories\"],\n",
        "    target=\"finishedfloor1squarefeet\",\n",
        "    tuning_params=parameters,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmp_col = df_prop.columns\n",
        "tmp_list = []\n",
        "\n",
        "for c in tmp_col:\n",
        "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.0:\n",
        "        tmp_list.append(c)\n",
        "\n",
        "print(\"Number of missing features: \", len(tmp_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Columns:\")\n",
        "print(df_prop.columns)\n",
        "print(\"Shape: \", df_prop.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='custom_features'></a>\n",
        "### Adding potentially useful custom features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some custom features are now created and added to the dataset that could intuitively be useful for building the final model and to better explain the data trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Features related to building properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Age of the building at the time of sale\n",
        "df_prop[\"yearbuilt\"] = pd.to_datetime(df_prop[\"yearbuilt\"], format=\"%Y\")\n",
        "df_prop[\"assessmentyear\"] = pd.to_datetime(df_prop[\"assessmentyear\"], format=\"%Y\")\n",
        "df_prop[\"Life-until-selling\"] = (\n",
        "    df_prop[\"transactiondate\"] - df_prop[\"yearbuilt\"]\n",
        ").dt.days\n",
        "\n",
        "# Relationship between structure value and land area\n",
        "df_prop[\"N-ValueProp\"] = (\n",
        "    df_prop[\"structuretaxvaluedollarcnt\"] / df_prop[\"landtaxvaluedollarcnt\"]\n",
        ")\n",
        "\n",
        "# Portion of livable area\n",
        "df_prop[\"N-LivingAreaProp\"] = (\n",
        "    df_prop[\"calculatedfinishedsquarefeet\"] / df_prop[\"lotsizesquarefeet\"]\n",
        ")\n",
        "\n",
        "# Amount of extra space\n",
        "df_prop[\"N-ExtraSpace\"] = (\n",
        "    df_prop[\"lotsizesquarefeet\"] - df_prop[\"calculatedfinishedsquarefeet\"]\n",
        ")\n",
        "\n",
        "# Features that indicate whether the property has a garage, pool, or jacuzzi and AC\n",
        "df_prop[\"N-GarPoolAC\"] = (\n",
        "    (df_prop[\"garagecarcnt\"] > 0)\n",
        "    & (df_prop[\"hashottuborspa\"] > 0)\n",
        "    & (df_prop[\"airconditioningtypeid\"] != 5)\n",
        ") * 1\n",
        "df_prop[\"N-GarPoolAC\"] = df_prop[\"N-GarPoolAC\"].astype(bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Home Tax to Total Tax Ratio by Assessment Year\n",
        "df_prop[\"N-ValueRatio\"] = df_prop[\"taxvaluedollarcnt\"] / df_prop[\"taxamount\"]\n",
        "\n",
        "# Total Tax Score\n",
        "df_prop[\"N-TaxScore\"] = df_prop[\"taxvaluedollarcnt\"] * df_prop[\"taxamount\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Location-related features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of properties per zip code\n",
        "zip_count = df_prop[\"regionidzip\"].value_counts().to_dict()\n",
        "df_prop[\"N-zip_count\"] = df_prop[\"regionidzip\"].map(zip_count)\n",
        "\n",
        "# Number of properties per city\n",
        "city_count = df_prop[\"regionidcity\"].value_counts().to_dict()\n",
        "df_prop[\"N-city_count\"] = df_prop[\"regionidcity\"].map(city_count)\n",
        "\n",
        "# Number of properties per country\n",
        "region_count = df_prop[\"regionidcounty\"].value_counts().to_dict()\n",
        "df_prop[\"N-county_count\"] = df_prop[\"regionidcounty\"].map(region_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Features that are the simplification of other features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Indicator whether AC is present or not\n",
        "df_prop[\"N-ACInd\"] = (df_prop[\"airconditioningtypeid\"] != 5) * 1\n",
        "df_prop[\"N-ACInd\"] = df_prop[\"N-ACInd\"].astype(bool)\n",
        "\n",
        "# Indicator whether heating is present or not\n",
        "df_prop[\"N-HeatInd\"] = (df_prop[\"heatingorsystemtypeid\"] != 13) * 1\n",
        "df_prop[\"N-HeatInd\"] = df_prop[\"N-HeatInd\"].astype(bool)\n",
        "\n",
        "# Type of land use for which the property is zoned - previously there were 25 categories, now they are compressed to 4\n",
        "df_prop[\"N-PropType\"] = df_prop[\"propertylandusetypeid\"].replace(\n",
        "    {\n",
        "        31: \"Mixed\",\n",
        "        46: \"Other\",\n",
        "        47: \"Mixed\",\n",
        "        246: \"Mixed\",\n",
        "        247: \"Mixed\",\n",
        "        248: \"Mixed\",\n",
        "        260: \"Home\",\n",
        "        261: \"Home\",\n",
        "        262: \"Home\",\n",
        "        263: \"Home\",\n",
        "        264: \"Home\",\n",
        "        265: \"Home\",\n",
        "        266: \"Home\",\n",
        "        267: \"Home\",\n",
        "        268: \"Home\",\n",
        "        269: \"Not Built\",\n",
        "        270: \"Home\",\n",
        "        271: \"Home\",\n",
        "        273: \"Home\",\n",
        "        274: \"Other\",\n",
        "        275: \"Home\",\n",
        "        276: \"Home\",\n",
        "        279: \"Home\",\n",
        "        290: \"Not Built\",\n",
        "        291: \"Not Built\",\n",
        "    }\n",
        ")\n",
        "df_prop.drop(columns=\"propertylandusetypeid\", inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Custom features related to 'structuretaxvaluedollarcnt' because it is considered an important specification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average structuretaxvaluedollarct per city\n",
        "group = (\n",
        "    df_prop.groupby(\"regionidcity\")[\"structuretaxvaluedollarcnt\"]\n",
        "    .aggregate(\"mean\")\n",
        "    .to_dict()\n",
        ")\n",
        "df_prop[\"N-Avg-structuretaxvaluedollarcnt\"] = df_prop[\"regionidcity\"].map(\n",
        "    group\n",
        ")  # assign 'regionidcity' to the mean calculated above and put into a dictionary with key 'regionidcity'\n",
        "\n",
        "# Deviation of the value from the mean\n",
        "df_prop[\"N-Dev-structuretaxvaluedollarcnt\"] = (\n",
        "    abs(\n",
        "        (\n",
        "            df_prop[\"structuretaxvaluedollarcnt\"]\n",
        "            - df_prop[\"N-Avg-structuretaxvaluedollarcnt\"]\n",
        "        )\n",
        "    )\n",
        "    / df_prop[\"N-Avg-structuretaxvaluedollarcnt\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In case there are \"Infinity\" values ​​in the dataset resulting from divisions by zero, these are set to 0 as it is semantically correct.\n",
        "df_prop.replace([np.inf, -np.inf], 0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop.info(max_cols=TOP_K_DISPLAY)\n",
        "print(\"Shape: \", df_prop.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='features_selection'></a>\n",
        "### Features importance, feature selection and preparation of the final dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point the dataset no longer has missing values ​​and intuitively the multicollinearity problems between the input features, highlighted in the previous analyses, have been resolved. The feature selection process can then be carried out in order to build even more accurate forecasting models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_var_names = set(\n",
        "    [\n",
        "        \"airconditioningtypeid\",\n",
        "        \"heatingorsystemtypeid\",\n",
        "        \"propertycountylandusecode\",\n",
        "        \"N-PropType\",\n",
        "        \"propertyzoningdesc\",\n",
        "        \"regionidcity\",\n",
        "        \"regionidcounty\",\n",
        "        \"regionidneighborhood\",\n",
        "        \"regionidzip\",\n",
        "        \"fips\",\n",
        "        \"rawcensustractandblock\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "for c in cat_var_names:\n",
        "    print(c, len(df_prop[c].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, it is decided to remove some of the categorical variables with many distinct elements (specifically 'propertyzoningdesc', 'propertycountylandusecode', 'regionidneighborhood', 'regionidzip', 'regionidcity', 'rawcensustractandblock') so as not to exponentially increase the size of the matrix after the OneHotEncoding process (also possible curse of dimensionality problem) and increase the time required for training and testing the models.<br>\n",
        "Please note that this encoding process is necessary for the correct management of categorical features, otherwise they would be interpreted in a numerical or ordinal way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "removed_cat = set(\n",
        "    [\n",
        "        \"propertyzoningdesc\",\n",
        "        \"propertycountylandusecode\",\n",
        "        \"regionidneighborhood\",\n",
        "        \"regionidzip\",\n",
        "        \"regionidcity\",\n",
        "        \"rawcensustractandblock\",\n",
        "    ]\n",
        ")\n",
        "one_hot_colmuns = list(cat_var_names.difference(removed_cat))\n",
        "\n",
        "\n",
        "one_hot_enc = OneHotEncoder(sparse_output=False)\n",
        "one_hot_enc.fit(df_prop[one_hot_colmuns])\n",
        "one_hot_tranform_name = one_hot_enc.get_feature_names_out(one_hot_colmuns)\n",
        "\n",
        "df_one_hot = pd.DataFrame(\n",
        "    one_hot_enc.transform(df_prop[one_hot_colmuns]), columns=one_hot_tranform_name # type: ignore\n",
        ")\n",
        "\n",
        "df_prop_drop_cat = df_prop.drop(columns=list(cat_var_names))\n",
        "df_prop_final = pd.concat(\n",
        "    [df_prop_drop_cat.reset_index(), df_one_hot.reset_index()], axis=1\n",
        ")\n",
        "df_prop_final.drop(columns=[\"index\"], inplace=True)\n",
        "\n",
        "\n",
        "# Convert date to integer\n",
        "df_prop_final[\"yearbuilt\"] = df_prop_final[\"yearbuilt\"].dt.year\n",
        "df_prop_final[\"assessmentyear\"] = df_prop_final[\"assessmentyear\"].dt.year\n",
        "to_float64_float32(df_prop_final)\n",
        "to_int64_int32(df_prop_final)\n",
        "del df_prop, df_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_prop_final.info(max_cols=TOP_K_DISPLAY)\n",
        "print(\"Final shape: \", df_prop_final.shape)\n",
        "df_prop_final.to_parquet(os.path.join(OUTPUT_DATA_DIR, \"final_df_2016.parquet\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_prop_final.drop(columns=[\"parcelid\", \"logerror\", \"transactiondate\"])\n",
        "y = df_prop_final[\"logerror\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "X and y will contain respectively:\n",
        "- the features to use for building the model\n",
        "- the actual values ​​to be used for the supervised learning task\n",
        "\n",
        "*Please note that the variables only contain data of type float (or convertible to float) because these are the only types accepted by the ML algorithms used*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the section below, we proceed to the tuning of the \"n_estimators\" parameter to build a model using gradient boosting that, once trained, will be used to select the features considered to have the greatest impact by the algorithm<br>\n",
        "It is also noted that the XGBoost library was chosen for its high efficiency and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuning_params = {\"n_estimators\": [i for i in range(1, 26, 1)]}\n",
        "\n",
        "X_train_80, X_test, y_train_80, y_test = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "xgb_grid = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=tuning_params,\n",
        "    cv=5,\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        "    verbose=0,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "print(\"Tuning XGBoost hyperparameters:\")\n",
        "xgb_grid.fit(X_train_80, y_train_80)\n",
        "print(\"Best Score: {:.4f}\".format(-xgb_grid.best_score_))\n",
        "print(\"Best Params: \", xgb_grid.best_params_)\n",
        "\n",
        "test_mse = root_mean_squared_error(y_true=y_test, y_pred=xgb_grid.predict(X_test))\n",
        "print(\"MSE: {:.4f}\".format(test_mse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_importance(\n",
        "    xgb_grid.best_estimator_.feature_importances_, X.columns, \"XGBoost Model \", limit=40\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*You can see from the graph regarding the features importance that many of the custom variables added previously have a significant weight in the construction of the model*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The presumed best subset of n features is now chosen using the subsequent feature ranking method with recursive feature elimination using cross-validation sets in order to reduce, as anticipated, the dimensionality of the data frame and the consequent problems of multicollinearity and curse of dimensionality to further optimize the predictions.<br>\n",
        "It was decided to set a minimum number of 10 features so that the final model does not have too few variables that would consequently not be able to explain the data accurately enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selector = RFECV(\n",
        "    xgb_grid.best_estimator_,\n",
        "    step=1,\n",
        "    cv=5,\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        "    min_features_to_select=10,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "selector.fit(X, y)\n",
        "selector.n_features_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The features selected by the algorithm are therefore 12, specifically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_important_features = [\n",
        "    x[0] for x in list(zip(X.columns, selector.support_)) if x[1]\n",
        "]\n",
        "print(list_important_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset containing the selected features is created and saved offline so that it can then be used for the construction of the two final models or for any future analyses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_important_X = X[list_important_features]\n",
        "selected_important_X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_important_X.to_parquet(OUTPUT_DATA_DIR + \"final_df_2016_filtered.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='mod_1'></a>\n",
        "### Building the model using Random Forest and tuning its parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this stage it was decided to implement parameter tuning in a more sophisticated way.\n",
        "Specifically, the method with cross validation sets will search for the best combination of \"n_estimators\", \"max_leaf_nodes\" and \"min_samples_leaf\", 3 of the main hyperparameters for the construction of the regressor using the Random Forest ensembling method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_80, X_test, y_train_80, y_test = train_test_split(\n",
        "    selected_important_X, y, test_size=0.20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuning_params = {\n",
        "    \"n_estimators\": [i for i in range(10, 81, 10)],\n",
        "    \"max_leaf_nodes\": [\n",
        "        10,\n",
        "        30,\n",
        "        50,\n",
        "        100,\n",
        "        200,\n",
        "    ],  # Grow trees with max_leaf_nodes in best-first fashion\n",
        "    \"min_samples_leaf\": [i for i in range(1, 5, 1)],\n",
        "}  # The minimum number of samples required to be at a leaf node.\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "rf_model = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=tuning_params,\n",
        "    cv=5,\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        "    verbose=0,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "print(\"Tuning Random Forest hyperparameters:\")\n",
        "rf_model.fit(X_train_80, y_train_80)\n",
        "print(\"Best Score: {:.4f}\".format(-rf_model.best_score_))\n",
        "print(\"Best Params: \", rf_model.best_params_)\n",
        "\n",
        "test_mse = root_mean_squared_error(y_true=y_test, y_pred=rf_model.predict(X_test))\n",
        "print(\"MSE: {:.4f}\".format(test_mse))\n",
        "\n",
        "joblib.dump(rf_model, os.path.join(MODEL_DIR, \"random_forest_model.pkl\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is believed that the Mean Squared Error values ​​obtained, especially in the testing set, are sufficiently low and that consequently the model using Random Forest is quite accurate. Furthermore, using cross validation sets has limited the possibility of overfitting of the predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_TESTS = 2\n",
        "step = 2\n",
        "offset = 10\n",
        "\n",
        "stats = np.array([])\n",
        "n_trees = [\n",
        "    1 if i == 0 else i\n",
        "    for i in range(0, rf_model.best_params_[\"n_estimators\"] + offset, step)\n",
        "]\n",
        "\n",
        "for l in n_trees:\n",
        "    y_preds = np.array([])\n",
        "\n",
        "    for i in range(N_TESTS):\n",
        "        Xs, ys = resample(X_train_80, y_train_80, n_samples=int(0.67 * len(y_train_80)))  # type: ignore\n",
        "\n",
        "        rf = RandomForestRegressor(\n",
        "            n_estimators=l,\n",
        "            max_leaf_nodes=rf_model.best_params_[\"max_leaf_nodes\"],\n",
        "            min_samples_leaf=rf_model.best_params_[\"min_samples_leaf\"],\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "        rf.fit(Xs, ys)\n",
        "\n",
        "        y_pred = rf.predict(X_test)\n",
        "        y_preds = np.column_stack([y_preds, y_pred]) if y_preds.size else y_pred\n",
        "\n",
        "    dt_bias = (y_test - np.mean(y_preds, axis=1)) ** 2\n",
        "    dt_variance = np.var(y_preds, axis=1)\n",
        "    dt_error = (y_preds - y_test.reshape(-1, 1)) ** 2.0\n",
        "\n",
        "    run_stats = np.array([dt_error.mean(), dt_bias.mean(), dt_variance.mean()])\n",
        "\n",
        "    stats = np.column_stack([stats, run_stats]) if stats.size else run_stats\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.set_title(\"Bias-Variance Decomposition Analysis - Random Forest\")\n",
        "ax.plot(n_trees, stats[0, :], \"o:\", label=\"Error\")\n",
        "ax.plot(n_trees, stats[1, :], \"o:\", label=\"Bias$^2$\")\n",
        "ax.plot(n_trees, stats[2, :], \"o:\", label=\"Variance\")\n",
        "ax.set_xlabel(\"Number of Trees\")\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "\n",
        "print(\"Error/Bias/Variance at the last iteration:\", stats[:, -1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen from the graph *number of trees - total error*, the error decreases (and the score improves) as the number of estimators used increases. In particular, it is clear, as per theoretical intuition, that since the predictor is based on a forest of trees, each of these by definition is sought to be fully grown, therefore with low distortion and high variance, and that the latter will then be reduced with the ensemble. In this specific case, it is highlighted how it is possible to significantly reduce the variance already using only 4 estimators.<br>\n",
        "We then recall that the formula for the decomposition of the total error is:\n",
        "$$\n",
        "Error(M) = Bias^2 + Variance + Noise\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='mod_2'></a>\n",
        "### Construction of the model exploiting Gradient Boosting and tuning of its parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even for the construction of this model it was decided to perform a more sophisticated tuning of the parameters.\n",
        "Specifically, the method with cross validation sets will search for the best combination of \"n_estimators\", \"max_leaves\" and \"learning_rate\", 3 of the main hyperparameters for the construction of the regressor using the Gradient Boosting method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_80, X_test, y_train_80, y_test = train_test_split(\n",
        "    selected_important_X, y, test_size=0.20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuning_params = {\n",
        "    \"n_estimators\": [i for i in range(10, 101, 10)],\n",
        "    \"max_leaves\": [\n",
        "        2,\n",
        "        5,\n",
        "        10,\n",
        "        50,\n",
        "        100,\n",
        "        200,\n",
        "    ],  # Maximum number of leaves; 0 indicates no limit\n",
        "    \"learning_rate\": [0.1, 0.2, 0.3, 0.4],\n",
        "}  # Boosting learning rate (xgb's “eta”).\n",
        "# Step size shrinkage used in update to prevents overfitting. The value must be between 0 and 1. Default is 0.3.\n",
        "\n",
        "xgb_m = xgb.XGBRegressor()\n",
        "xgb_model = GridSearchCV(\n",
        "    estimator=xgb_m,\n",
        "    param_grid=tuning_params,\n",
        "    cv=5,\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        "    verbose=0,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "print(\"Tuning XGBoost hyperparameters:\")\n",
        "xgb_model.fit(X_train_80, y_train_80)\n",
        "print(\"Best Score: {:.4f}\".format(-xgb_model.best_score_))\n",
        "print(\"Best Params: \", xgb_model.best_params_)\n",
        "\n",
        "test_mse = root_mean_squared_error(y_true=y_test, y_pred=xgb_model.predict(X_test))\n",
        "print(\"MSE: {:.4f}\".format(test_mse))\n",
        "\n",
        "joblib.dump(xgb_model, os.path.join(MODEL_DIR, \"xgboost_model.pkl\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also in this case it is believed that the Mean Squared Error values ​​obtained, in particular in the testing set, are sufficiently low and that consequently the model based on Gradient Boosting is quite accurate. Furthermore, using the cross validation sets has limited the possibility of overfitting in the regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_TESTS = 2\n",
        "step = 2\n",
        "offset = 10\n",
        "\n",
        "stats = np.array([])\n",
        "n_trees = [\n",
        "    1 if i == 0 else i\n",
        "    for i in range(0, xgb_model.best_params_[\"n_estimators\"] + offset, step)\n",
        "]\n",
        "\n",
        "for l in n_trees:\n",
        "    y_preds = np.array([])\n",
        "\n",
        "    for i in range(N_TESTS):\n",
        "        Xs, ys = resample(X_train_80, y_train_80, n_samples=int(0.67 * len(y_train_80)))  # type: ignore\n",
        "\n",
        "        xgb_m = xgb.XGBRegressor(\n",
        "            n_estimators=l,\n",
        "            learning_rate=xgb_model.best_params_[\"learning_rate\"],\n",
        "            max_leaves=xgb_model.best_params_[\"max_leaves\"],\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "        xgb_m.fit(Xs, ys)\n",
        "\n",
        "        y_pred = xgb_m.predict(X_test)\n",
        "        y_preds = np.column_stack([y_preds, y_pred]) if y_preds.size else y_pred\n",
        "\n",
        "    dt_bias = (y_test - np.mean(y_preds, axis=1)) ** 2\n",
        "    dt_variance = np.var(y_preds, axis=1)\n",
        "    dt_error = (y_preds - y_test.reshape(-1, 1)) ** 2.0\n",
        "\n",
        "    run_stats = np.array([dt_error.mean(), dt_bias.mean(), dt_variance.mean()])\n",
        "\n",
        "    stats = np.column_stack([stats, run_stats]) if stats.size else run_stats\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.set_title(\"Bias-Variance Decomposition Analysis - XGBoost\")\n",
        "ax.plot(n_trees, stats[0, :], \"o:\", label=\"Error\")\n",
        "ax.plot(n_trees, stats[1, :], \"o:\", label=\"Bias$^2$\")\n",
        "ax.plot(n_trees, stats[2, :], \"o:\", label=\"Variance\")\n",
        "ax.set_xlabel(\"Number of Trees\")\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "\n",
        "print(\"Error/Bias/Variance at the last iteration:\", stats[:, -1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen from the graph, also in this case the error decreases (and the score improves) as the number of estimators used increases. In particular, it is clear, as per theoretical intuition, that since the predictor is based on tree boosting, each of these by definition is sought to be small in size, therefore with low variance but high distortion, and that the latter will then be reduced with the ensemble.\n",
        "In this specific case the decomposition shows a nearly zero variance and an error that decreases exponentially as the distortion decreases, which can also be significantly reduced using only 10 estimators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='comparison_and_analysis'></a>\n",
        "### Comparison and analysis of models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the construction of the final statistical models has been completed, we now proceed to analyse their behaviour in more depth, in particular by focusing on some of the buildings where the given log error is the worst and others where it is the best.\n",
        "\n",
        "The buildings with log error between the most extreme value and the most extreme value minus 1 are then extracted (worst items) and ten of the buildings where the log error is zero (best items)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.displot(df_prop_final, x=\"logerror\", height=6, aspect=16 / 9);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variable 'logerror' follows a normal distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "offset = 1\n",
        "max_error_pos = df_prop_final[\"logerror\"].max() - offset\n",
        "max_error_neg = df_prop_final[\"logerror\"].min() + offset\n",
        "\n",
        "worst_items = pd.concat(\n",
        "    [\n",
        "        df_prop_final[df_prop_final[\"logerror\"] >= max_error_pos],\n",
        "        df_prop_final[df_prop_final[\"logerror\"] <= max_error_neg],\n",
        "    ]\n",
        ")\n",
        "best_items = df_prop_final[df_prop_final[\"logerror\"] == 0].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "worst_items_X = selected_important_X.iloc[worst_items.index]\n",
        "worst_items_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_items_X = selected_important_X.iloc[best_items.index]\n",
        "best_items_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(\n",
        "    range(len(worst_items)),\n",
        "    worst_items[\"logerror\"],\n",
        "    color=[\"# 9dd866\"],\n",
        "    label=\"Target value\",\n",
        ")\n",
        "plt.bar(\n",
        "    range(len(worst_items)),\n",
        "    rf_model.predict(worst_items_X),\n",
        "    color=[\"# ffa056\"],\n",
        "    label=\"RF value\",\n",
        ")\n",
        "plt.bar(\n",
        "    range(len(worst_items)),\n",
        "    xgb_model.predict(worst_items_X),\n",
        "    color=[\"# 0b84a5\"],\n",
        "    label=\"XGB value\",\n",
        ")\n",
        "plt.legend()\n",
        "plt.title(\"Log-errors comparisons - Worst items\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the bar chart of the predictions on problematic items, it can be noted that the estimates of the two final models created are very far from the target value calculated by the Zestimate estimator. This is probably due to the presence of many missing or outlier values, treated with analysis, recovery and correction techniques different from those used by the Zestimate team.\n",
        "However, it is assumed a priori that items with a high absolute log error may present anomalies or non-standard values ​​for some features, since the competition estimator itself returns predicted values ​​that are quite distant from the real log(SalePrice).\n",
        "\n",
        "To verify these hypotheses, it is thought that it is necessary to contact experts in the field of Data Science, the Zestimate team and real estate technicians."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(\n",
        "    range(len(best_items)),\n",
        "    best_items[\"logerror\"],\n",
        "    color=\"# 9dd866\",\n",
        "    label=\"Target value\",\n",
        ")\n",
        "plt.plot(\n",
        "    range(len(best_items)),\n",
        "    rf_model.predict(best_items_X),\n",
        "    color=\"# ffa056\",\n",
        "    label=\"RF value\",\n",
        ")\n",
        "plt.plot(\n",
        "    range(len(best_items)),\n",
        "    xgb_model.predict(best_items_X),\n",
        "    color=\"# 0b84a5\",\n",
        "    label=\"XGB value\",\n",
        ")\n",
        "plt.legend()\n",
        "plt.title(\"Log-errors comparisons - Best Items\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the line chart relating to the forecasts on items whose log error is zero, the two models created also return satisfactory values: in fact, the deviation from the target value of Zestimate is in the order of cents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In general, even in the graphs relating to best items and worst items, no big differences in terms of accuracy are noted between the model with Random Forest and the one with Gradient Boosting, even if the MSE value obtained in the testing set of the first model is slightly lower than the one returned by the second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='final'></a>\n",
        "### Final considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The various steps carried out are then briefly summarized:\n",
        "- Dataset analysis, feature engineering and missing value management\n",
        "- Features selection and model construction\n",
        "- Analysis and comparison of the obtained models\n",
        "\n",
        "In conclusion, thanks to this task it was possible to test on a real, extended and complex dataset, the performances of some Machine Learning methods such as Random Forest and Gradient Boosting, both exploiting decision trees but using different intuitions.<br>\n",
        "However, as previously mentioned, there is not enough evidence to prefer one model over the other since both return accurate and efficient predictions and the deviation in the value of the evaluation metric for the testing set is very low.\n",
        "Generally, predictors that use the Gradient Boosting technique perform better than those that exploit Random Forest by definition since they have as their objective the minimization of a loss function and the characteristic of additively building the various trees, even if they are more prone to overfitting.<br>\n",
        "Probably, with a deeper knowledge of the data, of their meaning, of their real trend and with the addition of external information to the dataset it is possible to obtain even more precise results and to build the optimal regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### *Summary table containing the evaluation data*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
