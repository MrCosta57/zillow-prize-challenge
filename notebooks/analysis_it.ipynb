{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROGETTO DWM 2021\n",
    "### Giovanni Costa - 880892"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indice:\n",
    "- [Analisi del dataset \"Train\"](#analisi_train)\n",
    "- [Analisi del dataset \"Properties\"](#analisi_prop)\n",
    "- [Features Engineering](#features_engineer)\n",
    "    - [Gestione dei missing values e rimozione delle colonne non necessarie o che presentano multicollinearità](#missing_val)\n",
    "        - [Recupero di missing values per features connesse a posizione geografica e tassazione](#recover_missing_pos_tax)\n",
    "    - [Aggiunta di features custom potenzialmente utili](#custom_features) \n",
    "- [Features importance, features selection e preparazione del dataset finale](#features_selection)\n",
    "- [Corstruzione del modello sfruttante Random Forest e tuning dei suoi parametri](#mod_1)\n",
    "- [Corstruzione del modello sfruttante Gradient Boosting e tuning dei suoi parametri](#mod_2)\n",
    "- [Comparazione e analisi dei modelli](#comparison_and_analisys)\n",
    "- [Considerazioni finali](#final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.general_utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings, random, joblib, os\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the working directory\n",
    "INPUT_DATA_DIR = \"../data/input\"\n",
    "OUTPUT_DATA_DIR = \"../data/output\"\n",
    "MODEL_DIR = \"models\"\n",
    "PROPS_FILENAME = \"properties_2016.parquet\"\n",
    "TRAIN_DATA_FILENAME = \"train_2016_v2.csv\"\n",
    "PROPS_PATH = os.path.join(INPUT_DATA_DIR, PROPS_FILENAME)\n",
    "TRAIN_DATA_PATH = os.path.join(INPUT_DATA_DIR, TRAIN_DATA_FILENAME)\n",
    "SEED = 42\n",
    "TOP_K_DISPLAY = 20\n",
    "\n",
    "# Set up the random seed\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# Display option\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analisi_train'></a>\n",
    "### Analisi del dataset \"Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    TRAIN_DATA_PATH, parse_dates=[\"transactiondate\"], date_format=\"%Y-%m-%d\"\n",
    ")\n",
    "df_train[df_train.select_dtypes(np.float64).columns] = df_train.select_dtypes(\n",
    "    np.float64\n",
    ").astype(np.float32)\n",
    "df_train.info(max_cols=TOP_K_DISPLAY)\n",
    "print(\"Shape: \", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values ratio\n",
    "df_train.isna().sum() / df_train.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo che il dataset contiene 3 colonne:\n",
    "- Parcelid: id univoco che identifica ogni istanza di edificio\n",
    "- Logerror: indice che dovremmo utilizzare per verificare la bontà del nostro modello<br>\n",
    "Dal sito internet della competizione: *\"Zillow is asking you to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. The log error is defined as logerror=log(Zestimate)−log(SalePrice)\"*\n",
    "- Transactiondate: data di vendita reale o stimata per quell'istanza di edificio\n",
    "\n",
    "Inoltre non ci sono valori nulli nel dataset ma ci sono parcelid duplicati, anche se la coppia parcelid e transactiondate è univoca per ogni riga. Questo implica che ci sono dati di vendite reali (o previste) con riferimento allo stesso edificio in diverse giornate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analisi_prop'></a>\n",
    "### Analisi del dataset \"Properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop = pd.DataFrame([])\n",
    "if PROPS_PATH.endswith(\".csv\"):\n",
    "    df_prop = pd.read_csv(PROPS_PATH)\n",
    "    to_float64_float32(df_prop)\n",
    "    df_prop.to_parquet(\n",
    "        os.path.join(INPUT_DATA_DIR, PROPS_FILENAME.split(\".\")[0] + \".parquet\")\n",
    "    )\n",
    "else:\n",
    "    df_prop = pd.read_parquet(PROPS_PATH)\n",
    "\n",
    "# df_prop=pd.read_csv(input_folder+data_file_name, header=0) #parametro nrows=x per limitare il numero di righe\n",
    "\n",
    "df_prop.info(max_cols=TOP_K_DISPLAY)\n",
    "print(\"Shape: \", df_prop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values ratio\n",
    "df_prop.isna().sum().sort_values(ascending=False)[:TOP_K_DISPLAY] / df_prop.shape[\n",
    "    0\n",
    "] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values plot\n",
    "df_missing = df_prop.isnull().sum(axis=0).reset_index()\n",
    "df_missing.columns = [\"column_name\", \"missing_count\"]\n",
    "df_missing = df_missing.loc[df_missing[\"missing_count\"] > 0]\n",
    "df_missing = df_missing.sort_values(by=\"missing_count\")\n",
    "\n",
    "ind = np.arange(df_missing.shape[0])\n",
    "fig, ax = plt.subplots(figsize=(12, 18))\n",
    "rects = ax.barh(ind, df_missing[\"missing_count\"])\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(df_missing[\"column_name\"], rotation=\"horizontal\")\n",
    "ax.set_xlabel(\"Count of missing values\")\n",
    "ax.set_title(\"Number of missing values in each column\")\n",
    "plt.show()\n",
    "del df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonne contenti variabili categoriali sulla base della ducumentazione fornita su kaggle.com (usate successivamente)\n",
    "other_cols = [\n",
    "    \"parcelid\",\n",
    "    \"airconditioningtypeid\",\n",
    "    \"architecturalstyletypeid\",\n",
    "    \"buildingqualitytypeid\",\n",
    "    \"buildingclasstypeid\",\n",
    "    \"decktypeid\",\n",
    "    \"fips\",\n",
    "    \"fireplaceflag\",\n",
    "    \"hashottuborspa\",\n",
    "    \"heatingorsystemtypeid\",\n",
    "    \"pooltypeid10\",\n",
    "    \"pooltypeid2\",\n",
    "    \"pooltypeid7\",\n",
    "    \"propertycountylandusecode\",\n",
    "    \"propertylandusetypeid\",\n",
    "    \"propertyzoningdesc\",\n",
    "    \"rawcensustractandblock\",\n",
    "    \"censustractandblock\",\n",
    "    \"regionidcounty\",\n",
    "    \"regionidcity\",\n",
    "    \"regionidzip\",\n",
    "    \"regionidneighborhood\",\n",
    "    \"regionidzip\",\n",
    "    \"storytypeid\",\n",
    "    \"typeconstructiontypeid\",\n",
    "    \"assessmentyear\",\n",
    "    \"taxdelinquencyflag\",\n",
    "    \"taxdelinquencyyear\",\n",
    "    \"yearbuilt\",\n",
    "]\n",
    "numerical_cols = [x for x in df_prop.columns if x not in other_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione dei valori delle colonne di tipo non float, int o datetime\n",
    "not_float_col = df_prop.select_dtypes(exclude=[np.float32, np.int64]).columns\n",
    "for c in not_float_col:\n",
    "    print(\"Column: \" + c)\n",
    "    print(\"values: \", df_prop[c].unique()[:TOP_K_DISPLAY], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset presenta molte features e istanze:\n",
    "- alcune di queste aventi tipo float32 sono in realtà identificativi numerici di tipo intero associati a categorie descritte più approfonditamente nella documentazione di Kaggle.com\n",
    "- quelle di tipo object invece sono stringhe rappresentanti valori booleani oppure codici relativi a categorie\n",
    "\n",
    "Le varie colonne verranno quindi gestite successivamente per rappresentare la corretta semantica del dato.\n",
    "\n",
    "Il dataset inoltre non presenta duplicati né in parcelid né globalmente, ma contiene un numero molto elevato di valori mancanti: diverse colonne infatti presentano un missing ratio superiore al 97%. Nei capitoli successivi verrà illustrato come vengono gestiti tali valori mancanti per ognuna delle features presenti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per questo particolare task di supervised learning sono sufficienti solamente le abitazioni all'interno del file \"train_2016_v2\" e non tutte quelle presenti in \"properties_2016\", si procede quindi all'unione dei due dataset analizzati precedentemente in modo da averne uno unico contenente l'insieme di tutte le colonne e solamente le righe che verranno effettivamente utilizzate per la previsione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop.info(max_cols=TOP_K_DISPLAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset dopo il merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop = df_train.merge(df_prop, on=\"parcelid\")\n",
    "df_prop.info(max_cols=TOP_K_DISPLAY)\n",
    "print(\"Shape: \", df_prop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features_engineer'></a>\n",
    "## Features Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='missing_val'></a>\n",
    "### Gestione dei missing values e rimozione delle colonne non necessarie o che presentano multicollinearità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap per visualizzare le correlazioni tra le variabili\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data=df_prop[numerical_cols].corr(), cmap=\"Reds\")\n",
    "plt.show()\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dall'analisi della heatmap sulla correlazione tra le variabili numeriche si nota che le features 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15' e 'finishedsquarefeet6' sono molto correlate essendo di colore rosso scuro.\n",
    "La stessa considerazione si applica anche a 'finishedfloor1squarefeet' e 'finishedsquarefeet50' che in aggiunta nella documentazione fornita hanno la stessa descrizione.\n",
    "Similmente anche 'bathroomcnt', 'calculatedbathnbr' e 'fullbathcnt' sono correlate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene mantenuta solamente 'calculatedfinishedsquarefeet' tra le quattro perché presenta meno valori missing, mentre tra 'finishedfloor1squarefeet' e 'finishedsquarefeet50' viene rimossa arbitrariamente 'finishedsquarefeet50' in quanto il missing ratio è equivalente.\n",
    "\n",
    "Vengono eliminati anche 'bathroomcnt' e 'calculatedbathnbr' lasciando 'bathroomcnt', in quanto viene applicata un'intuizione analoga alla precedente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[\n",
    "    [\n",
    "        \"calculatedfinishedsquarefeet\",\n",
    "        \"finishedsquarefeet12\",\n",
    "        \"finishedsquarefeet13\",\n",
    "        \"finishedsquarefeet15\",\n",
    "        \"finishedsquarefeet6\",\n",
    "    ]\n",
    "].isna().sum() / df_prop.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop.drop(\n",
    "    columns=[\n",
    "        \"finishedsquarefeet12\",\n",
    "        \"finishedsquarefeet13\",\n",
    "        \"finishedsquarefeet15\",\n",
    "        \"finishedsquarefeet6\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[\n",
    "    [\"finishedfloor1squarefeet\", \"finishedsquarefeet50\"]\n",
    "].isna().sum() / df_prop.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop.drop(columns=\"finishedsquarefeet50\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[\n",
    "    [\"calculatedbathnbr\", \"bathroomcnt\", \"fullbathcnt\"]\n",
    "].isna().sum() / df_prop.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop.drop(columns=[\"calculatedbathnbr\", \"fullbathcnt\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'hashottuborspa' e 'pooltypeid10' presentano semanticamente la stessa descrizione. Si decide di rimuovere 'pooltypeid10' che presenta meno valori missing.\n",
    "\n",
    "Viene inoltre assunto che se il valore di pool/hot tub (features 'pooltypeid2', 'pooltypeid7', 'poolcnt') non è presente questo indichi 0 elementi presenti nell'edificio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prop[\"hashottuborspa\"].value_counts())\n",
    "print(df_prop[\"pooltypeid10\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[[\"hashottuborspa\", \"pooltypeid10\"]].isnull().sum() / df_prop.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop.drop(columns=\"pooltypeid10\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[[\"pooltypeid2\", \"pooltypeid7\", \"poolcnt\"]] = df_prop[\n",
    "    [\"pooltypeid2\", \"pooltypeid7\", \"poolcnt\"]\n",
    "].fillna(0)\n",
    "df_prop[\"hashottuborspa\"] = df_prop[\"hashottuborspa\"].fillna(0)  # >90% nan\n",
    "df_prop[\"hashottuborspa\"] = df_prop[\"hashottuborspa\"].astype(bool);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per quanto riguarda la features 'poolsizesum' viene utilizzata la mediana dei valori presenti alle righe in cui la colonna 'poolcnt' è meggiore di 0, altrimenti questo valore viene impostato a 0\n",
    "(si sceglie di usare la mediana come filler perchè meno influenzata dagli outliers e perchè si assume che le dimensioni delle piscine negli Stati Uniti siano più o meno standard) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_poolsize = df_prop[df_prop[\"poolcnt\"] > 0][\"poolsizesum\"].median()\n",
    "df_prop.loc[\n",
    "    (df_prop[\"poolcnt\"] > 0) & (df_prop[\"poolsizesum\"].isna()), \"poolsizesum\"\n",
    "] = median_poolsize\n",
    "\n",
    "# Se non ha la piscina la dimensione della piscina è 0\n",
    "df_prop.loc[(df_prop[\"poolcnt\"] == 0), \"poolsizesum\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'fireplaceflag' e 'fireplacecnt' presentano delle inconsistenze:\n",
    "come si nota dall'analisi sottostante ci sono righe dove 'fireplacecnt' è presente, mentre 'fireplaceflag' è missing oppure errato. \n",
    "\n",
    "Si prosegue dunque mettendo a 0 i valori di 'fireplacecnt' quando NaN e impostando a True 'fireplaceflag' quando 'fireplacecnt' è presente, False altrimenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[\"fireplaceflag\"] = df_prop[\"fireplaceflag\"].fillna(0)  # >90% nan\n",
    "df_prop[\"fireplaceflag\"] = df_prop[\"fireplaceflag\"].astype(bool)\n",
    "df_prop.loc[(~df_prop[\"fireplacecnt\"].isna()), \"fireplaceflag\"] = True\n",
    "df_prop[\"fireplacecnt\"] = df_prop[\"fireplacecnt\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono riempiti i valori di 'taxdelinquencyflag', 'garagecarcnt', 'garagetotalsqft' assumendo 0 se il valore è NaN, come fatto in precedenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[[\"taxdelinquencyflag\", \"garagecarcnt\", \"garagetotalsqft\"]] = df_prop[\n",
    "    [\"taxdelinquencyflag\", \"garagecarcnt\", \"garagetotalsqft\"]\n",
    "].fillna(0)\n",
    "df_prop[\"taxdelinquencyflag\"] = df_prop[\"taxdelinquencyflag\"].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le features come 'airconditioningtypeid', 'heatingorsystemtypeid',  'threequarterbathnbr' invece si ritengono poco importanti e variabili così si decide di rimpiazzare i valori missing di tali features categoriali con la loro rispettiva moda.<br>\n",
    "Come si nota dai grafici sottastanti la frequenza di queste variabili categoriali è intuitivamente corretta, ragionevolmente si ipotizza infatti che AC e Heating System siano più comunemente 'Central' e che la maggior parte delle abitazioni abbiano solamente un bagno a trequarti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "df_prop[\"airconditioningtypeid\"].astype(int, errors=\"ignore\").hist(grid=False)\n",
    "plt.show()\n",
    "\n",
    "mode = float(df_prop[\"airconditioningtypeid\"].mode()[0])\n",
    "print(\"Moda: \", mode)\n",
    "df_prop[\"airconditioningtypeid\"] = df_prop[\"airconditioningtypeid\"].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "df_prop[\"heatingorsystemtypeid\"].astype(int, errors=\"ignore\").hist(grid=False)\n",
    "plt.show()\n",
    "\n",
    "mode = float(df_prop[\"heatingorsystemtypeid\"].mode()[0])\n",
    "print(\"Moda: \", mode)\n",
    "df_prop[\"heatingorsystemtypeid\"] = df_prop[\"heatingorsystemtypeid\"].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "df_prop[\"threequarterbathnbr\"].astype(int, errors=\"ignore\").hist(grid=False)\n",
    "plt.show()\n",
    "\n",
    "mode = float(df_prop[\"threequarterbathnbr\"].mode()[0])\n",
    "print(\"Moda: \", mode)\n",
    "df_prop[\"threequarterbathnbr\"] = df_prop[\"threequarterbathnbr\"].fillna(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si decide infine ora di rimuovere le features con missing ratio maggiore di 97% perché ritenute con troppa poca informazione per essere utili al task di regressione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_col = df_prop.columns\n",
    "for c in tmp_col:\n",
    "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.97:\n",
    "        df_prop.drop(columns=c, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_col = df_prop.columns\n",
    "tmp_list = []\n",
    "\n",
    "for c in tmp_col:\n",
    "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.0:\n",
    "        tmp_list.append(c)\n",
    "\n",
    "print(\"Number of features with missing values: \", len(tmp_list))\n",
    "print(tmp_list)\n",
    "print(\"Number of total features after droping: \", len(df_prop.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si analizzano quindi le features rimanenti rimanenti cercando di ripristinare gli ultimi missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recover_missing_pos_tax'></a>\n",
    "### Recupero di missing values per features connesse a posizione geografica e tassazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_col_names = [\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"buildingqualitytypeid\",\n",
    "    \"propertycountylandusecode\",\n",
    "    \"propertyzoningdesc\",\n",
    "    \"regionidcity\",\n",
    "    \"regionidneighborhood\",\n",
    "    \"regionidzip\",\n",
    "    \"unitcnt\",\n",
    "    \"yearbuilt\",\n",
    "]\n",
    "df_geo=df_prop[geo_col_names]\n",
    "df_geo.isna().sum() / df_geo.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitivamente, vista inoltre l'assenza di valori mancanti, si potrebbero utilizzare le features 'latitude' e 'longitude' relativi alla posizione dell'edificio per recuperare altri attributi non presenti: abitazioni geograficamente vicine infatti si pensa che possano avere determinate caratteristiche simili<br><br>\n",
    "Si ripristinano quindi i valori originali di queste due features perché come descritto nella documentazine fornita quelli all'interno del dataset sono moltiplicati per 10^6.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[\"latitude\"] = df_prop[\"latitude\"] / (10**6)\n",
    "df_prop[\"longitude\"] = df_prop[\"longitude\"] / (10**6)\n",
    "# df_prop.dropna( axis = 0, subset = ['latitude', 'longitude'], inplace = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si è deciso di utilizzare l'algoritmo K-nearest neighbors (KNN) per svolgere il task di recupero perché ritenuto il più adatto visto che si basa sull'apprendimento mediante analogie tra istanze vicine e vista la sua efficienza computazionale. Nella cella di codice sottostante sono inoltre spiegate le motivazioni e le ipotesi usate per il ripristino delle variabili categoriali e numeriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[\"buildingqualitytypeid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "parameters = {\"n_neighbors\": [1, 2, 3, 4, 5, 8, 10]}\n",
    "\n",
    "\n",
    "# Si ipotizza che blocchi di case vicine siano state costruite più o meno tutte nello stesso periodo e che quindi abbiano qualità simile\n",
    "fillna_knn(\n",
    "    df=df_prop,\n",
    "    base=[\"latitude\", \"longitude\"],\n",
    "    target=\"buildingqualitytypeid\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "\n",
    "# Abitazioni vicine hanno lo stesso countrylandusecode\n",
    "tmp_label_enc = zoningcode2int(df=df_prop, target=\"propertycountylandusecode\")\n",
    "fillna_knn(\n",
    "    df=df_prop,\n",
    "    base=[\"latitude\", \"longitude\"],\n",
    "    target=\"propertycountylandusecode\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "df_prop[\"propertycountylandusecode\"] = tmp_label_enc.inverse_transform(\n",
    "    df_prop[\"propertycountylandusecode\"].astype(int)\n",
    ")\n",
    "\n",
    "\n",
    "# Abitazioni vicine hanno la stessa zoning description\n",
    "tmp_label_enc = zoningcode2int(df=df_prop, target=\"propertyzoningdesc\")\n",
    "fillna_knn(\n",
    "    df=df_prop,\n",
    "    base=[\"latitude\", \"longitude\"],\n",
    "    target=\"propertyzoningdesc\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "df_prop[\"propertyzoningdesc\"] = tmp_label_enc.inverse_transform(\n",
    "    df_prop[\"propertyzoningdesc\"].astype(int)\n",
    ")\n",
    "\n",
    "\n",
    "# Anche in questo caso proprietà vicine si assume abbiano gli stessi regionidcity, regionidneighborhood e regionidzip\n",
    "fillna_knn(\n",
    "    df=df_prop,\n",
    "    base=[\"latitude\", \"longitude\"],\n",
    "    target=\"regionidcity\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "\n",
    "fillna_knn(\n",
    "    df=df_prop,\n",
    "    base=[\"latitude\", \"longitude\"],\n",
    "    target=\"regionidneighborhood\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "\n",
    "fillna_knn(\n",
    "    df=df_prop,\n",
    "    base=[\"latitude\", \"longitude\"],\n",
    "    target=\"regionidzip\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "\n",
    "\n",
    "# Stessa intuizione per i campi 'unitcnt' (Number of units the structure is built into), 'yearbuilt' e 'lotsizesquarefeet'\n",
    "fillna_knn(\n",
    "    df=df_prop,\n",
    "    base=[\"latitude\", \"longitude\"],\n",
    "    target=\"unitcnt\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "\n",
    "fillna_knn(\n",
    "    df=df_prop,\n",
    "    base=[\"latitude\", \"longitude\"],\n",
    "    target=\"yearbuilt\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "\n",
    "fillna_knn_reg(\n",
    "    df=df_prop,\n",
    "    base=[\"latitude\", \"longitude\"],\n",
    "    target=\"lotsizesquarefeet\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(action='default', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per quanto riguarda la feature 'finishedfloor1squarefeet', come si può anche notare dalla heatmap all'inizio della sezione corrente, questa è correlata con 'calculatedfinishedsquarefeet'. Si prova quindi ad utilizzare quest'ultima per svolgerne il filling dei valori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 13))\n",
    "sns.jointplot(\n",
    "    x=df_prop[\"finishedfloor1squarefeet\"].values,\n",
    "    y=df_prop[\"calculatedfinishedsquarefeet\"].values,\n",
    ")\n",
    "plt.ylabel(\"calculatedfinishedsquarefeet\", fontsize=10)\n",
    "plt.xlabel(\"finishedfloor1squarefeet\", fontsize=10)\n",
    "plt.title(\"finishedfloor1squarefeet Vs calculatedfinishedsquarefeet\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal grafico si nota che in alcune abitazioni i valori delle features sono esattamente gli stessi: probabilmente alcune case hanno la loro area complessiva occupata da solamente una stanza (come potrebbe essere una sorta di studio). Vengono quindi assunte queste informazioni per vere e si procede al riempimento.<br>\n",
    "Inoltre alcune righe del dataset contengono valori di 'finishedfloor1squarefeet' maggiori della dimensione totale dell'abitazione, probabilmente dovuto ad un inserimento del dato in input errato. Si decide quindi di rimuovere tali righe dal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop.loc[\n",
    "    (df_prop[\"finishedfloor1squarefeet\"].isna()) & (df_prop[\"numberofstories\"] == 1),\n",
    "    \"finishedfloor1squarefeet\",\n",
    "] = df_prop.loc[\n",
    "    (df_prop[\"finishedfloor1squarefeet\"].isna()) & (df_prop[\"numberofstories\"] == 1),\n",
    "    \"calculatedfinishedsquarefeet\",\n",
    "]\n",
    "\n",
    "droprows = df_prop.loc[\n",
    "    df_prop[\"calculatedfinishedsquarefeet\"] < df_prop[\"finishedfloor1squarefeet\"]\n",
    "].index\n",
    "df_prop = df_prop.drop(droprows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_col = df_prop.columns\n",
    "tmp_list = []\n",
    "\n",
    "for c in tmp_col:\n",
    "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.0:\n",
    "        tmp_list.append(c)\n",
    "\n",
    "print(\"Number of features with missing values: \", len(tmp_list))\n",
    "print(tmp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si gestiscono ora le variabili inerenti alle tasse sugli edifici: in particolare si prova a recuperare i valori di 'structuretaxvaluedollarcnt', 'taxamount' e 'landtaxvaluedollarcnt'.\n",
    "La variabile 'taxvaluedollarcnt' si ipotizza essere la più significativa da usare come supporto in quanto contenente anche minor numero di valori missing.<br>\n",
    "\n",
    "Viene svolto quindi il filling dei valori NaN di 'taxvaluedollarcnt' usando la sua mediana, in modo da avere un risultato meno sensibile agli outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_col_names = [\n",
    "    \"taxvaluedollarcnt\",\n",
    "    \"landtaxvaluedollarcnt\",\n",
    "    \"structuretaxvaluedollarcnt\",\n",
    "    \"taxamount\",\n",
    "]\n",
    "df_tax = df_prop[tax_col_names]\n",
    "df_tax.isna().sum() / df_tax.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_prop[\"taxvaluedollarcnt\"].median()\n",
    "df_prop[\"taxvaluedollarcnt\"] = df_prop[\"taxvaluedollarcnt\"].fillna(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da analisi su correlazione e grafici delle distribuzioni per le tre variabili target si nota inoltre che 'taxvaluedollarcnt' è la variabile più correlata per tutte: si prova quindi a svolgere una predizione dei valori missing usando l'algoritmo KNN per le features 'structuretaxvaluedollarcnt', 'taxamount' e 'landtaxvaluedollarcnt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_tax.corr()\n",
    "\n",
    "print(\"Target: structuretaxvaluedollarcnt\")\n",
    "print(x[\"structuretaxvaluedollarcnt\"].sort_values(ascending=False))\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.jointplot(\n",
    "    x=df_tax[\"structuretaxvaluedollarcnt\"].values, y=df_tax[\"taxvaluedollarcnt\"].values\n",
    ")\n",
    "plt.ylabel(\"taxvaluedollarcnt\", fontsize=12)\n",
    "plt.xlabel(\"structuretaxvaluedollarcnt\", fontsize=12)\n",
    "plt.title(\"structuretaxvaluedollarcnt Vs taxvaluedollarcnt\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "print(\"Target: taxamount\")\n",
    "print(x[\"taxamount\"].sort_values(ascending=False))\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.jointplot(x=df_tax[\"taxamount\"].values, y=df_tax[\"taxvaluedollarcnt\"].values)\n",
    "plt.ylabel(\"taxvaluedollarcnt\", fontsize=12)\n",
    "plt.xlabel(\"taxamount\", fontsize=12)\n",
    "plt.title(\"taxamount Vs taxvaluedollarcnt\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "print(\"Target: landtaxvaluedollarcnt\")\n",
    "print(x[\"landtaxvaluedollarcnt\"].sort_values(ascending=False))\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.jointplot(\n",
    "    x=df_tax[\"landtaxvaluedollarcnt\"].values, y=df_tax[\"taxvaluedollarcnt\"].values\n",
    ")\n",
    "plt.ylabel(\"taxvaluedollarcnt\", fontsize=12)\n",
    "plt.xlabel(\"landtaxvaluedollarcnt\", fontsize=12)\n",
    "plt.title(\"landtaxvaluedollarcnt Vs taxvaluedollarcnt\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_neighbors\": [10, 20, 30, 40, 50, 100]}\n",
    "fillna_knn_reg(\n",
    "    df=df_prop,\n",
    "    base=[\"taxvaluedollarcnt\"],\n",
    "    target=\"structuretaxvaluedollarcnt\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "fillna_knn_reg(\n",
    "    df=df_prop, base=[\"taxvaluedollarcnt\"], target=\"taxamount\", tuning_params=parameters\n",
    ")\n",
    "fillna_knn_reg(\n",
    "    df=df_prop,\n",
    "    base=[\"taxvaluedollarcnt\"],\n",
    "    target=\"landtaxvaluedollarcnt\",\n",
    "    tuning_params=parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_col = df_prop.columns\n",
    "tmp_list = []\n",
    "\n",
    "for c in tmp_col:\n",
    "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.0:\n",
    "        tmp_list.append(c)\n",
    "\n",
    "print(\"Number of features with missing values: \", len(tmp_list))\n",
    "print(tmp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può notare restano solamente poche features con valori missing, si procede quindi:\n",
    "- riempiendo 'numberofstories' con la sua moda\n",
    "- rimuovendo la colonna 'censustractandblock' perché si assume che l'attributo 'rawcensustractandblock' contenga le stesse informazioni anche se in maniera meno elaborata.\n",
    "- costruendo un predittore per 'calculatedfinishedsquarefeet' basato su 'bathroomcnt', 'bedroomcnt', 'structuretaxvaluedollarcnt'. Si ipotizza infatti che la dimensione totale dell'area abitabile sia dipendente da quanti bagni e camere da letto ci sono e da quante tasse sulla struttura dell'abitazione sono attribuite.\n",
    "- costruendo un predittore per 'finishedfloor1squarefeet' basato sulle stesse features precedenti e sul numero di piani dell'edificio (numberofstories). Si assume che il numero di piani presenti possa essere utile per calcolare la features 'finishedfloor1squarefeet' (*Dimensioni della superficie abitativa finita al primo piano (ingresso) della casa*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "x_min = df_prop[\"numberofstories\"].min()\n",
    "x_max = df_prop[\"numberofstories\"].max()\n",
    "plt.xlim([x_min, x_max])\n",
    "plt.xticks(np.arange(x_min, x_max + 1, 1))\n",
    "df_prop[\"numberofstories\"].hist()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "mode = float(df_prop[\"numberofstories\"].mode()[0])\n",
    "print(\"Moda: \", mode)\n",
    "df_prop[\"numberofstories\"] = df_prop[\"numberofstories\"].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop.drop(columns=\"censustractandblock\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna_knn_reg(\n",
    "    df=df_prop,\n",
    "    base=[\"bathroomcnt\", \"bedroomcnt\", \"structuretaxvaluedollarcnt\"],\n",
    "    target=\"calculatedfinishedsquarefeet\",\n",
    "    tuning_params=parameters,\n",
    ")\n",
    "fillna_knn_reg(\n",
    "    df=df_prop,\n",
    "    base=[\"bathroomcnt\", \"bedroomcnt\", \"structuretaxvaluedollarcnt\", \"numberofstories\"],\n",
    "    target=\"finishedfloor1squarefeet\",\n",
    "    tuning_params=parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_col = df_prop.columns\n",
    "tmp_list = []\n",
    "\n",
    "for c in tmp_col:\n",
    "    if df_prop[c].isna().sum() / df_prop.shape[0] > 0.0:\n",
    "        tmp_list.append(c)\n",
    "\n",
    "print(\"Number of missing features: \", len(tmp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\")\n",
    "print(df_prop.columns)\n",
    "print(\"Shape: \", df_prop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='custom_features'></a>\n",
    "### Aggiunta di features custom potenzialmente utili"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono ora create ed aggiunte al dataset alcune features custom che intuitivamente potrebbero essere utili per la costruzione del modello finale e per spiegare meglio l'andamento dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features relative a proprietà dell'edificio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Età dell'edificio al momento della vendita\n",
    "df_prop[\"yearbuilt\"] = pd.to_datetime(df_prop[\"yearbuilt\"], format=\"%Y\")\n",
    "df_prop[\"assessmentyear\"] = pd.to_datetime(df_prop[\"assessmentyear\"], format=\"%Y\")\n",
    "df_prop[\"Life-until-selling\"] = (\n",
    "    df_prop[\"transactiondate\"] - df_prop[\"yearbuilt\"]\n",
    ").dt.days\n",
    "\n",
    "# Rapporto tra structure value e land area\n",
    "df_prop[\"N-ValueProp\"] = (\n",
    "    df_prop[\"structuretaxvaluedollarcnt\"] / df_prop[\"landtaxvaluedollarcnt\"]\n",
    ")\n",
    "\n",
    "# Porzione di area vivibile\n",
    "df_prop[\"N-LivingAreaProp\"] = (\n",
    "    df_prop[\"calculatedfinishedsquarefeet\"] / df_prop[\"lotsizesquarefeet\"]\n",
    ")\n",
    "\n",
    "# Quantità di spazio extra\n",
    "df_prop[\"N-ExtraSpace\"] = (\n",
    "    df_prop[\"lotsizesquarefeet\"] - df_prop[\"calculatedfinishedsquarefeet\"]\n",
    ")\n",
    "\n",
    "# Features che indica se la proprietà ha garage, piscina, o ibromassaggio e AC\n",
    "df_prop[\"N-GarPoolAC\"] = (\n",
    "    (df_prop[\"garagecarcnt\"] > 0)\n",
    "    & (df_prop[\"hashottuborspa\"] > 0)\n",
    "    & (df_prop[\"airconditioningtypeid\"] != 5)\n",
    ") * 1\n",
    "df_prop[\"N-GarPoolAC\"] = df_prop[\"N-GarPoolAC\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapporto tasse sulla casa su tasse totali per assesment year\n",
    "df_prop[\"N-ValueRatio\"] = df_prop[\"taxvaluedollarcnt\"] / df_prop[\"taxamount\"]\n",
    "\n",
    "# Total Tax Score\n",
    "df_prop[\"N-TaxScore\"] = df_prop[\"taxvaluedollarcnt\"] * df_prop[\"taxamount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features relative alla posizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero di proprietà per zip code\n",
    "zip_count = df_prop[\"regionidzip\"].value_counts().to_dict()\n",
    "df_prop[\"N-zip_count\"] = df_prop[\"regionidzip\"].map(zip_count)\n",
    "\n",
    "# Numero di proprietà per città\n",
    "city_count = df_prop[\"regionidcity\"].value_counts().to_dict()\n",
    "df_prop[\"N-city_count\"] = df_prop[\"regionidcity\"].map(city_count)\n",
    "\n",
    "# Numero di proprietà per per paese\n",
    "region_count = df_prop[\"regionidcounty\"].value_counts().to_dict()\n",
    "df_prop[\"N-county_count\"] = df_prop[\"regionidcounty\"].map(region_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features che sono la semplificazione di altre features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicatore se AC è presente o no\n",
    "df_prop[\"N-ACInd\"] = (df_prop[\"airconditioningtypeid\"] != 5) * 1\n",
    "df_prop[\"N-ACInd\"] = df_prop[\"N-ACInd\"].astype(bool)\n",
    "\n",
    "# Indicatore se riscaldamento è presente o no\n",
    "df_prop[\"N-HeatInd\"] = (df_prop[\"heatingorsystemtypeid\"] != 13) * 1\n",
    "df_prop[\"N-HeatInd\"] = df_prop[\"N-HeatInd\"].astype(bool)\n",
    "\n",
    "# Tipo di destinazione d'uso del terreno per il quale è suddiviso in zone l'immobile - prima erano 25 categorie, ora vengono compresse a 4\n",
    "df_prop[\"N-PropType\"] = df_prop[\"propertylandusetypeid\"].replace(\n",
    "    {\n",
    "        31: \"Mixed\",\n",
    "        46: \"Other\",\n",
    "        47: \"Mixed\",\n",
    "        246: \"Mixed\",\n",
    "        247: \"Mixed\",\n",
    "        248: \"Mixed\",\n",
    "        260: \"Home\",\n",
    "        261: \"Home\",\n",
    "        262: \"Home\",\n",
    "        263: \"Home\",\n",
    "        264: \"Home\",\n",
    "        265: \"Home\",\n",
    "        266: \"Home\",\n",
    "        267: \"Home\",\n",
    "        268: \"Home\",\n",
    "        269: \"Not Built\",\n",
    "        270: \"Home\",\n",
    "        271: \"Home\",\n",
    "        273: \"Home\",\n",
    "        274: \"Other\",\n",
    "        275: \"Home\",\n",
    "        276: \"Home\",\n",
    "        279: \"Home\",\n",
    "        290: \"Not Built\",\n",
    "        291: \"Not Built\",\n",
    "    }\n",
    ")\n",
    "df_prop.drop(columns=\"propertylandusetypeid\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features custom relative a 'structuretaxvaluedollarcnt' perché viene considerata una specifica importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Media di structuretaxvaluedollarcnt per città\n",
    "group = (\n",
    "    df_prop.groupby(\"regionidcity\")[\"structuretaxvaluedollarcnt\"]\n",
    "    .aggregate(\"mean\")\n",
    "    .to_dict()\n",
    ")\n",
    "df_prop[\"N-Avg-structuretaxvaluedollarcnt\"] = df_prop[\"regionidcity\"].map(\n",
    "    group\n",
    ")  # assegna 'regionidcity' alla media calcolata sopra e messa in un dizionario con chiave 'regionidcity'\n",
    "\n",
    "# Discostamento del valore dalla media\n",
    "df_prop[\"N-Dev-structuretaxvaluedollarcnt\"] = (\n",
    "    abs(\n",
    "        (\n",
    "            df_prop[\"structuretaxvaluedollarcnt\"]\n",
    "            - df_prop[\"N-Avg-structuretaxvaluedollarcnt\"]\n",
    "        )\n",
    "    )\n",
    "    / df_prop[\"N-Avg-structuretaxvaluedollarcnt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nel caso siano presenti valori \"Infinito\" nel dataset derivanti da divisioni per zero questi vengono posti a 0 in quanto semanticamente corretto\n",
    "df_prop.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop.info(max_cols=TOP_K_DISPLAY)\n",
    "print(\"Shape: \", df_prop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='features_selection'></a>\n",
    "### Features importance, features selection e preparazione del dataset finale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrivati a questo punto il dataset non presenta più valori mancanti e intuitivamente sono stati risolti i problemi di multicollinearità tra le features date in input, evidenziati nelle analisi precedenti. Può essere quindi svolto il processo di features selection in modo da costruire modelli previsionali ancora più accurati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var_names = set(\n",
    "    [\n",
    "        \"airconditioningtypeid\",\n",
    "        \"heatingorsystemtypeid\",\n",
    "        \"propertycountylandusecode\",\n",
    "        \"N-PropType\",\n",
    "        \"propertyzoningdesc\",\n",
    "        \"regionidcity\",\n",
    "        \"regionidcounty\",\n",
    "        \"regionidneighborhood\",\n",
    "        \"regionidzip\",\n",
    "        \"fips\",\n",
    "        \"rawcensustractandblock\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for c in cat_var_names:\n",
    "    print(c, len(df_prop[c].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si decide però di rimuovere alcune delle variabili categoriali con molti elementi distinti (nello specifico 'propertyzoningdesc', 'propertycountylandusecode', 'regionidneighborhood', 'regionidzip', 'regionidcity', 'rawcensustractandblock') in modo da non aumentare esponenzialmente la dimensione della matrice dopo il processo di OneHotEncoding (possibile anche problema di curse of dimensionality) ed innalzare il tempo richiesto per il training e il testing dei modelli.<br>\n",
    "Si fa presente che questo processo di encoding è necessario per la corretta gestione di features categriali, altrimenti queste verrebbero interpretate in modo numerico o ordinale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_cat = set(\n",
    "    [\n",
    "        \"propertyzoningdesc\",\n",
    "        \"propertycountylandusecode\",\n",
    "        \"regionidneighborhood\",\n",
    "        \"regionidzip\",\n",
    "        \"regionidcity\",\n",
    "        \"rawcensustractandblock\",\n",
    "    ]\n",
    ")\n",
    "one_hot_colmuns = list(cat_var_names.difference(removed_cat))\n",
    "\n",
    "\n",
    "one_hot_enc = OneHotEncoder(sparse_output=False)\n",
    "one_hot_enc.fit(df_prop[one_hot_colmuns])\n",
    "one_hot_tranform_name = one_hot_enc.get_feature_names_out(one_hot_colmuns)\n",
    "\n",
    "df_one_hot = pd.DataFrame(\n",
    "    one_hot_enc.transform(df_prop[one_hot_colmuns]), columns=one_hot_tranform_name # type: ignore\n",
    ")\n",
    "\n",
    "df_prop_drop_cat = df_prop.drop(columns=list(cat_var_names))\n",
    "df_prop_final = pd.concat(\n",
    "    [df_prop_drop_cat.reset_index(), df_one_hot.reset_index()], axis=1\n",
    ")\n",
    "df_prop_final.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "\n",
    "# Convert date to integer\n",
    "df_prop_final[\"yearbuilt\"] = df_prop_final[\"yearbuilt\"].dt.year\n",
    "df_prop_final[\"assessmentyear\"] = df_prop_final[\"assessmentyear\"].dt.year\n",
    "to_float64_float32(df_prop_final)\n",
    "to_int64_int32(df_prop_final)\n",
    "del df_prop, df_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop_final.info(max_cols=TOP_K_DISPLAY)\n",
    "print(\"Final shape: \", df_prop_final.shape)\n",
    "df_prop_final.to_parquet(os.path.join(OUTPUT_DATA_DIR, \"final_df_2016.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_prop_final.drop(columns=[\"parcelid\", \"logerror\", \"transactiondate\"])\n",
    "y = df_prop_final[\"logerror\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X e y conterranno rispettivamente:\n",
    "- le features da usare per la costruzione del modello\n",
    "- i valori reali da utilizzare per il task di supervised learning\n",
    "\n",
    "*Si fa notare che le variabili contengono solo dati di tipo float (o convertibili in float) perché unici tipi accettati dagli algoritmi di ML utilizzati*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si procede nella sezione sottostante al tuning del parametro \"n_estimators\" per la costruzione di un modello sfruttante gradient boosting che una volta allenato verrà utilizzato per selezionare le features ritenute di maggior impatto dall'algoritmo<br>\n",
    "Si fa notare inoltre che è stata scelta la libreria XGBoost per la sua elevata efficienza ed accuratezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_params = {\"n_estimators\": [i for i in range(1, 26, 1)]}\n",
    "\n",
    "X_train_80, X_test, y_train_80, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_grid = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=tuning_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"Tuning XGBoost hyperparameters:\")\n",
    "xgb_grid.fit(X_train_80, y_train_80)\n",
    "print(\"Best Score: {:.4f}\".format(-xgb_grid.best_score_))\n",
    "print(\"Best Params: \", xgb_grid.best_params_)\n",
    "\n",
    "test_mse = root_mean_squared_error(y_true=y_test, y_pred=xgb_grid.predict(X_test))\n",
    "print(\"MSE: {:.4f}\".format(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(\n",
    "    xgb_grid.best_estimator_.feature_importances_, X.columns, \"XGBoost Model \", limit=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Si nota dal grafico riguardante la features importance che molte delle variabili custom aggiunte precedentemente hanno un peso rilevante nella costruzione del modello*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene ora scelto mediante il successivo metodo di feature ranking con recursive feature elimination usante cross-validation sets il presunto miglior sottinsieme di n features in modo da ridurre, come anticipato, la dimensionalità del dataframe ed i conseguenti problemi di multicollinearità e curse of dimentionality per ottimizzare maggiormente le previsioni.<br>\n",
    "Si è deciso di impostare un numero minimo di 10 features in modo che nel modello finale non rimangano troppe poche variabili che di conseguenza non riuscirebbero a spiegare in modo sufficientemente accurato i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFECV(\n",
    "    xgb_grid.best_estimator_,\n",
    "    step=1,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    min_features_to_select=10,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "selector.fit(X, y)\n",
    "selector.n_features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le features selezionate dall'algoritmo sono quindi 12, nello specifico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_important_features = [\n",
    "    x[0] for x in list(zip(X.columns, selector.support_)) if x[1]\n",
    "]\n",
    "print(list_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene creato e salvato offline il dataset contenete le features selezionate in modo che possa poi essere usato per la costruzione dei due modelli finali o per eventuali analisi future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_important_X = X[list_important_features]\n",
    "selected_important_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_important_X.to_parquet(OUTPUT_DATA_DIR + \"final_df_2016_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mod_1'></a>\n",
    "### Corstruzione del modello sfruttante Random Forest e tuning dei suoi parametri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa fase si è deciso di realizzare in modo più sofisticato il tuning degli parametri.\n",
    "Nello specifico il metodo con cross validation sets cercherà la migliore combinazione di \"n_estimators\", \"max_leaf_nodes\" e \"min_samples_leaf\", 3 degli iperparametri principali per la costruzione del regressore sfruttante il metodo di ensembling Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_80, X_test, y_train_80, y_test = train_test_split(\n",
    "    selected_important_X, y, test_size=0.20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_params = {\n",
    "    \"n_estimators\": [i for i in range(10, 81, 10)],\n",
    "    \"max_leaf_nodes\": [\n",
    "        10,\n",
    "        30,\n",
    "        50,\n",
    "        100,\n",
    "        200,\n",
    "    ],  # Grow trees with max_leaf_nodes in best-first fashion\n",
    "    \"min_samples_leaf\": [i for i in range(1, 5, 1)],\n",
    "}  # The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_model = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=tuning_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"Tuning Random Forest hyperparameters:\")\n",
    "rf_model.fit(X_train_80, y_train_80)\n",
    "print(\"Best Score: {:.4f}\".format(-rf_model.best_score_))\n",
    "print(\"Best Params: \", rf_model.best_params_)\n",
    "\n",
    "test_mse = root_mean_squared_error(y_true=y_test, y_pred=rf_model.predict(X_test))\n",
    "print(\"MSE: {:.4f}\".format(test_mse))\n",
    "\n",
    "joblib.dump(rf_model, os.path.join(MODEL_DIR, \"random_forest_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ritiene che i valori di Mean Squared Error ottenuti, in particolare nel testing set, siano sufficientemente bassi e che di conseguenza il modello usante Random Forest sia abbastanza accurato. Inoltre utilizzando i cross validation sets si è limitata la possibilità di overfitting del predittore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TESTS = 2\n",
    "step = 2\n",
    "offset = 10\n",
    "\n",
    "stats = np.array([])\n",
    "n_trees = [\n",
    "    1 if i == 0 else i\n",
    "    for i in range(0, rf_model.best_params_[\"n_estimators\"] + offset, step)\n",
    "]\n",
    "\n",
    "for l in n_trees:\n",
    "    y_preds = np.array([])\n",
    "\n",
    "    for i in range(N_TESTS):\n",
    "        Xs, ys = resample(X_train_80, y_train_80, n_samples=int(0.67 * len(y_train_80)))  # type: ignore\n",
    "\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=l,\n",
    "            max_leaf_nodes=rf_model.best_params_[\"max_leaf_nodes\"],\n",
    "            min_samples_leaf=rf_model.best_params_[\"min_samples_leaf\"],\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        rf.fit(Xs, ys)\n",
    "\n",
    "        y_pred = rf.predict(X_test)\n",
    "        y_preds = np.column_stack([y_preds, y_pred]) if y_preds.size else y_pred\n",
    "\n",
    "    dt_bias = (y_test - np.mean(y_preds, axis=1)) ** 2\n",
    "    dt_variance = np.var(y_preds, axis=1)\n",
    "    dt_error = (y_preds - y_test.reshape(-1, 1)) ** 2.0\n",
    "\n",
    "    run_stats = np.array([dt_error.mean(), dt_bias.mean(), dt_variance.mean()])\n",
    "\n",
    "    stats = np.column_stack([stats, run_stats]) if stats.size else run_stats\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_title(\"Bias-Variance Decomposition Analysis - Random Forest\")\n",
    "ax.plot(n_trees, stats[0, :], \"o:\", label=\"Error\")\n",
    "ax.plot(n_trees, stats[1, :], \"o:\", label=\"Bias$^2$\")\n",
    "ax.plot(n_trees, stats[2, :], \"o:\", label=\"Variance\")\n",
    "ax.set_xlabel(\"Number of Trees\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "print(\"Error/Bias/Variance at the last iteration:\", stats[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può notare dal grafico *numero alberi - errore totale*, l'errore diminuisce (e lo score migliora) all'aumetare del numero di stimatori utilizzati. In particolare si evince, come da intuizione teorica, che essendo il predittore basato su foresta di alberi ogniuno di questi per definizione si cerca sia fully grown, quindi con bassa distorsione e alta varianza, e che quest'ultima varrà poi ridotta con l'ensembing. In questo specifico caso è evidenziato come si riesca a ridurre la varianza in modo significativo già usando solamente 4 stimatori.<br>\n",
    "Si ricorda poi che la formula di decomposizione dell'errore totale è:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mod_2'></a>\n",
    "### Corstruzione del modello sfruttante Gradient Boosting e tuning dei suoi parametri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche per la costruzione di questo modello si è deciso di realizzare più sofisticatamente il tuning degli parametri.\n",
    "Nello specifico il metodo con cross validation sets cercherà la migliore combinazione di \"n_estimators\", \"max_leaves\" e \"learning_rate\", 3 degli iperparametri principali per la costruzione del regressore sfruttante il metodo Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_80, X_test, y_train_80, y_test = train_test_split(\n",
    "    selected_important_X, y, test_size=0.20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_params = {\n",
    "    \"n_estimators\": [i for i in range(10, 101, 10)],\n",
    "    \"max_leaves\": [\n",
    "        2,\n",
    "        5,\n",
    "        10,\n",
    "        50,\n",
    "        100,\n",
    "        200,\n",
    "    ],  # Maximum number of leaves; 0 indicates no limit\n",
    "    \"learning_rate\": [0.1, 0.2, 0.3, 0.4],\n",
    "}  # Boosting learning rate (xgb’s “eta”).\n",
    "# Step size shrinkage used in update to prevents overfitting. The value must be between 0 and 1. Default is 0.3.\n",
    "\n",
    "xgb_m = xgb.XGBRegressor()\n",
    "xgb_model = GridSearchCV(\n",
    "    estimator=xgb_m,\n",
    "    param_grid=tuning_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"Tuning XGBoost hyperparameters:\")\n",
    "xgb_model.fit(X_train_80, y_train_80)\n",
    "print(\"Best Score: {:.4f}\".format(-xgb_model.best_score_))\n",
    "print(\"Best Params: \", xgb_model.best_params_)\n",
    "\n",
    "test_mse = root_mean_squared_error(y_true=y_test, y_pred=xgb_model.predict(X_test))\n",
    "print(\"MSE: {:.4f}\".format(test_mse))\n",
    "\n",
    "joblib.dump(xgb_model, os.path.join(MODEL_DIR, \"xgboost_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso si ritiene che i valori di Mean Squared Error ottenuti, in particolare nel testing set, siano sufficientemente bassi e che di conseguenza il modello basato su Gradient Boosting sia abbastanza accurato. Inoltre utilizzando i cross validation sets si è limitata la possibilità di overfitting nel regressore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TESTS = 2\n",
    "step = 2\n",
    "offset = 10\n",
    "\n",
    "stats = np.array([])\n",
    "n_trees = [\n",
    "    1 if i == 0 else i\n",
    "    for i in range(0, xgb_model.best_params_[\"n_estimators\"] + offset, step)\n",
    "]\n",
    "\n",
    "for l in n_trees:\n",
    "    y_preds = np.array([])\n",
    "\n",
    "    for i in range(N_TESTS):\n",
    "        Xs, ys = resample(X_train_80, y_train_80, n_samples=int(0.67 * len(y_train_80)))  # type: ignore\n",
    "\n",
    "        xgb_m = xgb.XGBRegressor(\n",
    "            n_estimators=l,\n",
    "            learning_rate=xgb_model.best_params_[\"learning_rate\"],\n",
    "            max_leaves=xgb_model.best_params_[\"max_leaves\"],\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        xgb_m.fit(Xs, ys)\n",
    "\n",
    "        y_pred = xgb_m.predict(X_test)\n",
    "        y_preds = np.column_stack([y_preds, y_pred]) if y_preds.size else y_pred\n",
    "\n",
    "    dt_bias = (y_test - np.mean(y_preds, axis=1)) ** 2\n",
    "    dt_variance = np.var(y_preds, axis=1)\n",
    "    dt_error = (y_preds - y_test.reshape(-1, 1)) ** 2.0\n",
    "\n",
    "    run_stats = np.array([dt_error.mean(), dt_bias.mean(), dt_variance.mean()])\n",
    "\n",
    "    stats = np.column_stack([stats, run_stats]) if stats.size else run_stats\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_title(\"Bias-Variance Decomposition Analysis - XGBoost\")\n",
    "ax.plot(n_trees, stats[0, :], \"o:\", label=\"Error\")\n",
    "ax.plot(n_trees, stats[1, :], \"o:\", label=\"Bias$^2$\")\n",
    "ax.plot(n_trees, stats[2, :], \"o:\", label=\"Variance\")\n",
    "ax.set_xlabel(\"Number of Trees\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "print(\"Error/Bias/Variance at the last iteration:\", stats[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può notare dal grafico, anche in questo caso l'errore dimiuisce (e lo score migliora) all'aumetare del numero di stimatori utilizzato. In particolare si evince, come da intuizione teorica, che essendo il predittore basato su boosting di alberi, ogniuno di questi per definizione si cerca sia di dimensioni ridotte, quindi con bassa varianza ma alta distorsione, e che quest'ultima varrà varrà poi ridotta con l'ensembing. \n",
    "In questo specifico caso la decomposizione mostra una varianza quasi nulla ed un errore che descresce esponenzialmente al diminuire della distorzione, che si riesce inoltre a ridurre in modo significativo già usando solamente 10 stimatori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison_and_analisys'></a>\n",
    "### Comparazione e analisi dei modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimata la costruzione dei modelli statistici finali si porcede ora ad analizzarne più approfonditamente il comportamento, in particolare fecendo un focus su alcuni degli edifici dove il logerror dato è il peggiore ed altri dove è il migliore.\n",
    "\n",
    "Vengono quindi estratti gli edifici con log error compreso tra il valore più estremo e il valore più estremo meno 1 (worst items) e dieci tra le costruzioni dove il log error è zero (best items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_prop_final, x=\"logerror\", height=6, aspect=16 / 9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variabile 'logerror' segue una distribuzione normale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 1\n",
    "max_error_pos = df_prop_final[\"logerror\"].max() - offset\n",
    "max_error_neg = df_prop_final[\"logerror\"].min() + offset\n",
    "\n",
    "worst_items = pd.concat(\n",
    "    [\n",
    "        df_prop_final[df_prop_final[\"logerror\"] >= max_error_pos],\n",
    "        df_prop_final[df_prop_final[\"logerror\"] <= max_error_neg],\n",
    "    ]\n",
    ")\n",
    "best_items = df_prop_final[df_prop_final[\"logerror\"] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_items_X = selected_important_X.iloc[worst_items.index]\n",
    "worst_items_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_items_X = selected_important_X.iloc[best_items.index]\n",
    "best_items_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(\n",
    "    range(len(worst_items)),\n",
    "    worst_items[\"logerror\"],\n",
    "    color=[\"#9dd866\"],\n",
    "    label=\"Target value\",\n",
    ")\n",
    "plt.bar(\n",
    "    range(len(worst_items)),\n",
    "    rf_model.predict(worst_items_X),\n",
    "    color=[\"#ffa056\"],\n",
    "    label=\"RF value\",\n",
    ")\n",
    "plt.bar(\n",
    "    range(len(worst_items)),\n",
    "    xgb_model.predict(worst_items_X),\n",
    "    color=[\"#0b84a5\"],\n",
    "    label=\"XGB value\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Log-errors comparisons - Worst items\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal bar chart relativo alle previsioni su item problematici si può notare che le stime dei due modelli finali creati sono molto distanti dal valore target calcolato dallo stimatore Zestimate. Probabilmente questo è dovuto alla presenza di molti valori missing o outliers, trattati con tecniche di analisi, recupero e correzione differenti da quelle usate dal team di Zestimate.\n",
    "A priori però si ipotizza che gli item con logerror in valore assoluto elevato possano presentare anomalie o valori non standard per alcune features, in quanto proprio lo stimatore della competizione restituisce dei valori predetti abbastanza distanti dal log(SalePrice) reale.\n",
    "\n",
    "Per verificare queste ipotesi si pensa sia necessario contattare degli esperti in ambito Data Science, il team di Zestimate e dei tecnici immobiliari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(\n",
    "    range(len(best_items)),\n",
    "    best_items[\"logerror\"],\n",
    "    color=\"#9dd866\",\n",
    "    label=\"Target value\",\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(best_items)),\n",
    "    rf_model.predict(best_items_X),\n",
    "    color=\"#ffa056\",\n",
    "    label=\"RF value\",\n",
    ")\n",
    "plt.plot(\n",
    "    range(len(best_items)),\n",
    "    xgb_model.predict(best_items_X),\n",
    "    color=\"#0b84a5\",\n",
    "    label=\"XGB value\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Log-errors comparisons - Best Items\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel line chart inerente alle previsioni su item il cui log error è nullo invece anche i due modelli creati restituiscono valori soddisfacienti: infatti, il discostamento rispetto al valore target di Zestimate è nell'ordine dei centesini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In generale, anche nei grafici relativi a best items e worst items, non si notano grosse differenze in termini di accuratezza tra il modello con Random Forest e quello con Gradient Boosting, anche se il valore di MSE ottenuto nel testing set del primo modello e leggermente inferiore rispetto a quello restituito dal secondo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='final'></a>\n",
    "### Considerazioni finali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono quindi brevemente ricapitolati i vari passaggi svolti:\n",
    "- Analisi dei dataset, features engineering e gestione dei missing value\n",
    "- Features selection e costruzione dei modelli\n",
    "- Analisi e comparazione dei modelli ottenuti\n",
    "\n",
    "Concludendo, grazie a questo task si sono potuto testare su un dataset reale, esteso e complesso, le performance di alcuni metodi di Machine Learning come Random Forest e Gradient Boosting, entrambi sfruttanti alberi di decisione ma utilizzanti intuizioni diverse.<br>\n",
    "Tuttavia, come anticipato in precedenza, non si dispone di sufficienti elementi per preferire un modello rispetto all'altro in quanto entrambi ritornano previsioni accurate ed in modo efficiente e il discostamento nel valore della metrica di valutazione per il testing set è molto bassa.\n",
    "Generalmente, i predittori che utilizzano la tecnica Gradient Boosting performano meglio di quelli che sfruttano Random Forest per definizione in quanto hanno come obiettivo la minimizzazione di una funzione di perdita e la caratteristica di costruire additivamente i vari alberi, anche se sono più proni all'overfitting.<br>\n",
    "Probabilmente, con una conoscenza più approfondita dei dati, del loro significato, del loro andamento reale e con l'aggiunta di informazioni esterne al dataset è possibile arrivare ad ottenere risultati ancora più precisi ed alla costruzione del modello di regressione ottimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Tabella riassuntiva contenente i dati delle valutazioni*\n",
    "| Set type | Model | MSE Score |\n",
    "| --- | --- | --- |\n",
    "| Validation Sets | Random Forest | 0.0258 |\n",
    "| Validation Sets | Gradient Boosting | 0.0259 |\n",
    "| Testing Set | Random Forest | 0.0254 |\n",
    "| Testing Set | Gradient Boosting | 0.0257 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
