{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Datasets Analisys DWM 2021 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- Add custom features to your dataset (e.g., avg. prices in the same geographical\n",
    "area) and evaluate their contribution\n",
    "- Compare at least two prediction methods, after fine-tuning their parameters\n",
    "- Investigate instances with the most correct predictions and with the most wrong\n",
    "predictions. Do they have some peculiarities? Any strange feature distribution?\n",
    "- To get a good mark: more prediction methods, not trashing features, external\n",
    "data, other ML Libraries, prediction methods not discussed during the course, ecc\n",
    "\n",
    "Durante l'orale posso mostrare dei dati che non sono nella relazione di 5 pagine consegnata (es dei grafici particolari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Task: Home Value Prediction: https://www.kaggle.com/c/zillow-prize-1/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_folder='data/'\n",
    "data_file_name='properties_2017.parquet'\n",
    "#feature_dict_file_name='zillow_data_dictionary.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert file from CSV to Parquet\n",
    "'''df_houses=pd.read_csv(input_folder+properties_2017.csv, header=0)\n",
    "df_houses.to_parquet(input_folder+\"properties_2017.parquet\")''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(input_folder+\"train_2017.csv\")\n",
    "df_train.info()\n",
    "print('Shape: ', df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['parcelid'].duplicated().sum())\n",
    "print(df_train[['parcelid', 'transactiondate']].duplicated().sum()) #stessa casa venduta in date diverse con prezzi diversi\n",
    "#mi serve una feature/features che catturi questa diversità in base alla data (differenza di qualcosa, correlazioni ecc), oppure introdurre la data come features\n",
    "#es se la ho: data di costruzione della casa posso relazionare la \"vecchiaia\" in base alla data di vendita e il prezzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_dict=pd.read_excel(input_folder+feature_dict_file_name)\n",
    "df_houses=pd.read_parquet(input_folder+data_file_name)\n",
    "df_houses[df_houses.select_dtypes(np.float64).columns] = df_houses.select_dtypes(np.float64).astype(np.float32)\n",
    "#risparmio circa 600mb di memoria e probabilmente alcune operazioni sono più veloci \n",
    "\n",
    "df_houses.info()\n",
    "print('Shape: ', df_houses.shape)\n",
    "#può essere utile salvare la variabile con %store o salvando in un file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_houses.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['parcelid', 'hashottuborspa', 'propertycountylandusecode',\n",
       "       'propertyzoningdesc', 'fireplaceflag', 'taxdelinquencyflag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_houses.select_dtypes(exclude=[np.float32]).columns\n",
    "#possible categorical column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_houses.isna().sum()\n",
    "#NAN percentage:\n",
    "df_houses.isna().sum()/df_houses.shape[0] *100\n",
    "#se ci sono tanti tanti nan e la colonna non la considero importante la posso droppare\n",
    "#o posso rimpiazzare il valore e mettere una colonna con un flag se rimpiazzato o no, o usare random forest\n",
    "#per la colonne categoriali posso usare one hot encoding e se ho una categoria \"other\" posso aggregare insieme quella con il missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cerco di selezionare un dataset con pochi missing alleno e tutto fino alla fine e poi lo raffino\n",
    "#utile anche se uso random forest per \"iniziare\" le sostituzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_houses['parcelid'].duplicated().sum() è univoco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma se una istanza (riga) ha molti nan?\n",
    "Se nel train posso dropparla, nel test no\n",
    "\n",
    "Se ho tante categorie? Tipo la via?\n",
    "- Posso aggregare per macrocategorie (es per regione) così diminuisco\n",
    "- Oppure scelgo le 10 più frequenti ad esempio e le altre le metto a \"other\"\n",
    "- per ottimizzare se ho una variabile ordinale (alto, medio, basso), la posso codificare come interi ordinati ugualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso provare a destrutturare il dataset (ad esempio per regionidcountry) ed allenare 3 modelli per i 3 cluster trovati in base alla regione\n",
    "\n",
    "Provo a vedere qualcosa con PCA (Principal\n",
    " Component Analisys) con 2/3 componenti per disegnare con un grafico le features principali che contrubuiscono con la prima o la seconda componente (vedo come fatto a lezione)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERFORMANCE: Posso provare a campionare solo una parte del dataset e non tutto per efficienza di memoria (metà dataset, 1/4) e decidere il modello e in tuning. Una volta finito questo se basta la memoria provo ad allenarlo su tutto. Posso provare ad usare il notebook di Colab o di Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENSAMBLE DI ALGORITMI: https://www.kaggle.com/code/abhisheksharma26jan/zillowabhishek\n",
    "(nel video del 06/12 in realtà parla di un altro notebook)\n",
    "\n",
    "ENSAMBLE STACKING\n",
    "Modello che fa la media (o media pesata) dei modelli passati diversi.\n",
    "Il modello è fatto da istanze (case id) sulle righe e nome dell'alg sulle y, nelle celle c'è il valore grezzo predetto.\n",
    "La media pesata, cioè il valore dei pesi tra tutti è una regressione lineare, trovare la combinazione ottima di pesi dei vari modelli\n",
    "\n",
    "ENSAMBLE CASCADING\n",
    "Per ogni istanza uso un modello per decidere la strada da prendere e passare loutput un altro modello a cascata basato sulla strada scelta da quello prima. Es se ho una features costosa da calcolare creo un modello senza questa features, se per un istanza questo modello mi dice che ha senso calcolare la features gli passo l'istanza a sto modello con la features costosa e la calcolo, altrimenti no (uso un filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
